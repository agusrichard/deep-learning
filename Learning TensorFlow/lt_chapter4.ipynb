{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lt-chapter4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_8ob6B9ylEO",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 4\n",
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYadembamfz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z1zXc4t5NxO",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdgntNVXXnMD",
        "colab_type": "text"
      },
      "source": [
        "- The fundamental difference between fully connected and convolutional neural networks\n",
        "is the pattern of connections between consecutive layers. In the fully connected\n",
        "case, as the name might suggest, each unit is connected to all of the units in the previous\n",
        "layer.\n",
        "- In a convolutional layer of a neural network, on the other hand, each unit is connected\n",
        "to a (typically small) number of nearby units in the previous layer. Furthermore,\n",
        "all units are connected to the previous layer in the same way, with the exact same\n",
        "weights and structure.\n",
        "- In a nutshell all it\n",
        "means for us is applying a small “window” of weights (also known as filters) across an\n",
        "image.\n",
        "- Each convolutional layer\n",
        "looks at an increasingly larger part of the image as we go deeper into the network.\n",
        "Most commonly, this will be followed by fully connected layers that in the biologically\n",
        "inspired analogy act as the higher levels of visual processing dealing with global\n",
        "information.\n",
        "- The second angle, more hard fact engineering–oriented, stems from the nature of\n",
        "images and their contents. When looking for an object in an image, say the face of a\n",
        "cat, we would typically want to be able to detect it regardless of its position in the\n",
        "image. This reflects the property of natural images that the same content may be\n",
        "found in different locations of an image. This is property is known as an invariance—invariances of this sort can also be expected with respect to (small) rotations, changing\n",
        "lighting conditions, etc. Correspondingly, when building an object-recognition system, it should be invariant\n",
        "to translation (and, depending on the scenario, probably also rotation and deformations\n",
        "of many sorts, but that is another matter). Put simply, it therefore makes sense\n",
        "to perform the same exact computation on different parts of the image. In this view, a\n",
        "convolutional neural network layer computes the same features of an image, across\n",
        "all spatial areas.\n",
        "- Regularization is most often applied by adding implicit information\n",
        "regarding the desired results (this could take the form of saying\n",
        "we would rather have a smoother function, when searching a\n",
        "function space). In the convolutional neural network case, we\n",
        "explicitly state that we are looking for weights in a relatively lowdimensional\n",
        "subspace corresponding to fixed-size convolutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uejcu6dsb73y",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKCqgl8Eb_pG",
        "colab_type": "text"
      },
      "source": [
        "## MNIST: Take II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnB29h0scBg4",
        "colab_type": "text"
      },
      "source": [
        "### Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJHZUTFlcLq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.nn.conv2d(x, W, strides=[1, 1, , 1, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhgmM1_cYAY",
        "colab_type": "text"
      },
      "source": [
        "- Here, x is the data—the input image, or a downstream feature map obtained further\n",
        "along in the network, after applying previous convolution layers. \n",
        "- As discussed previously,\n",
        "in typical CNN models we stack convolutional layers hierarchically, and feature\n",
        "map is simply a commonly used term referring to the output of each such layer.\n",
        "Another way to view the output of these layers is as processed images, the result of\n",
        "applying a filter and perhaps some other operations. Here, this filter is parameterized\n",
        "by W, the learned weights of our network representing the convolution filter.\n",
        "- The output of this operation will depend on the shape of x and W, and in our case is\n",
        "four-dimensional. The image data x will be of shape: [None, 28, 28, 1]. Meaning that we have an unknown number of images, each 28×28 pixels and with\n",
        "one color channel (since these are grayscale images). The weights W we use will be of\n",
        "shape: [5, 5, 1, 32] where the initial 5×5×1 represents the size of the small “window” in the image to be\n",
        "convolved, in our case a 5×5 region. In images that have multiple color channels, we regard each image as a threedimensional\n",
        "tensor of RGB values, but in this one-channel data they are just twodimensional,\n",
        "and convolutional filters are applied to two-dimensional regions. Later,\n",
        "when we tackle the CIFAR10 data, we’ll see examples of multiple-channel images and\n",
        "how to set the size of weights W accordingly. The final 32 is the number of feature maps. In other words, we have multiple sets of\n",
        "weights for the convolutional layer—in this case, 32 of them. Recall that the idea of a\n",
        "convolutional layer is to compute the same feature along the image; we would simply\n",
        "like to compute many such features and thus use multiple sets of convolutional filters.\n",
        "- The strides argument controls the spatial movement of the filter W across the image\n",
        "(or feature map) x. The value [1, 1, 1, 1] means that the filter is applied to the input in one-pixel\n",
        "intervals in each dimension, corresponding to a “full” convolution. Other settings of\n",
        "this argument allow us to introduce skips in the application of the filter—a common\n",
        "practice that we apply later—thus making the resulting feature map smaller. \n",
        "- Finally, setting padding to 'SAME' means that the borders of x are padded such that\n",
        "the size of the result of the operation is the same as the size of x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDvCK0B0e0nL",
        "colab_type": "text"
      },
      "source": [
        "### Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1LWzKJRfWGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.nn.max_pool(x, ksize=[1, 2, 2, -1], strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-9fcaWafq3Y",
        "colab_type": "text"
      },
      "source": [
        "- The reasoning behind this is both technical and more theoretical. The technical\n",
        "aspect is that pooling reduces the size of the data to be processed downstream. This\n",
        "can drastically reduce the number of overall parameters in the model, especially if we\n",
        "use fully connected layers after the convolutional ones.\n",
        "- The more theoretical reason for applying pooling is that we would like our computed\n",
        "features not to care about small changes in position in an image. For instance, a feature\n",
        "looking for eyes in the top-right part of an image should not change too much if\n",
        "we move the camera a bit to the right when taking the picture, moving the eyes\n",
        "slightly to the center of the image. Aggregating the “eye-detector feature” spatially\n",
        "allows the model to overcome such spatial variability between images, capturing\n",
        "some form of invariance as discussed at the beginning of this chapter.\n",
        "- Max pooling outputs the maximum of the input in each region of a predefined size\n",
        "(here 2×2). The ksize argument controls the size of the pooling (2×2), and the\n",
        "strides argument controls by how much we “slide” the pooling grids across x, just as in the case of the convolution layer. Setting this to a 2×2 grid means that the output of\n",
        "the pooling will be exactly one-half of the height and width of the original, and in\n",
        "total one-quarter of the size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VURmOxyZhE4h",
        "colab_type": "text"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-p2weXWhoCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.nn.dropout(layer, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cGkdxHuhu9a",
        "colab_type": "text"
      },
      "source": [
        "The final element we will need for our model is dropout. This is a regularization trick\n",
        "used in order to force the network to distribute the learned representation across all\n",
        "the neurons. Dropout “turns off ” a random preset fraction of the units in a layer, by\n",
        "setting their values to zero during training. These dropped-out neurons are random\n",
        "—different for each computation—forcing the network to learn a representation that\n",
        "will work even after the dropout. This process is often thought of as training an\n",
        "“ensemble” of multiple networks, thereby increasing generalization. When using the\n",
        "network as a classifier at test time (“inference”), there is no dropout and the full network\n",
        "is used as is.\n",
        "\n",
        "In order to be able to change this value (which we must do, since for testing we would\n",
        "like this to be 1.0, meaning no dropout at all), we will use a tf.Variable and pass\n",
        "one value for train (.5) and another for test (1.0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMe3T-eyiZOp",
        "colab_type": "text"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3huptZNibpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], \n",
        "                          strides=[1, 2, 2, 1], padding='SAME')\n",
        "    \n",
        "def conv_layer(input, shape):\n",
        "    W = weight_variable(shape)\n",
        "    b = bias_variable([shape[3]])\n",
        "    return tf.nn.relu(conv2d(input, W) + b)\n",
        "    \n",
        "def full_layer(input, size):\n",
        "    in_size = int(input.get_shape()[1])\n",
        "    W = weight_variable([in_size, size])\n",
        "    b = bias_variable([size])\n",
        "    return tf.matmul(input, W) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgntdMVijme-",
        "colab_type": "text"
      },
      "source": [
        "weight_variable()\n",
        "\n",
        "    This specifies the weights for either fully connected or convolutional layers of the\n",
        "    network. They are initialized randomly using a truncated normal distribution\n",
        "    with a standard deviation of .1. This sort of initialization with a random normal\n",
        "    distribution that is truncated at the tails is pretty common and generally produces\n",
        "    good results (see the upcoming note on random initialization).\n",
        "\n",
        "bias_variable()\n",
        "\n",
        "    This defines the bias elements in either a fully connected or a convolutional layer.\n",
        "    These are all initialized with the constant value of .1.\n",
        "    conv2d()\n",
        "    This specifies the convolution we will typically use. A full convolution (no skips)\n",
        "    with an output the same size as the input.\n",
        "\n",
        "max_pool_2×2\n",
        "\n",
        "    This sets the max pool to half the size across the height/width dimensions, and in\n",
        "    total a quarter the size of the feature map.\n",
        "\n",
        "conv_layer()\n",
        "\n",
        "    This is the actual layer we will use. Linear convolution as defined in conv2d, with\n",
        "    a bias, followed by the ReLU nonlinearity.\n",
        "\n",
        "full_layer()\n",
        "\n",
        "    A standard full layer with a bias. Notice that here we didn’t add the ReLU. This\n",
        "    allows us to use the same layer for the final output, where we don’t need the nonlinear\n",
        "    part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAr775HWl0ih",
        "colab_type": "code",
        "outputId": "2c46518c-8863-42df-d9ce-f39d274ec6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "conv1 = conv_layer(x_image, shape=[5, 5, 1, 32])\n",
        "conv1_pool = max_pool_2x2(conv1)\n",
        "\n",
        "conv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\n",
        "conv2_pool = max_pool_2x2(conv2)\n",
        "\n",
        "conv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\n",
        "full_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
        "\n",
        "y_conv = full_layer(full1_drop, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-71f6fe22afb4>:15: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rou9W9j97ION",
        "colab_type": "text"
      },
      "source": [
        "We start by defining the placeholders for the images and correct labels, x and y_,\n",
        "respectively. Next, we reshape the image data into the 2D image format with size\n",
        "28×28×1. Recall we did not need this spatial aspect of the data for our previous\n",
        "MNIST model, since all pixels were treated independently, but a major source of\n",
        "power in the convolutional neural network framework is the utilization of this spatial\n",
        "meaning when considering images.\n",
        "\n",
        "Next we have two consecutive layers of convolution and pooling, each with 5×5 convolutions\n",
        "and 64 feature maps, followed by a single fully connected layer with 1,024\n",
        "units. Before applying the fully connected layer we flatten the image back to a single\n",
        "vector form, since the fully connected layer no longer needs the spatial aspect.\n",
        "\n",
        "Notice that the size of the image following the two convolution and pooling layers is\n",
        "7×7×64. The original 28×28 pixel image is reduced first to 14×14, and then to 7×7 in\n",
        "the two pooling operations. The 64 is the number of feature maps we created in the\n",
        "second convolutional layer. When considering the total number of learned parameters\n",
        "in the model, a large proportion will be in the fully connected layer (going from\n",
        "7×7×64 to 1,024 gives us 3.2 million parameters). This number would have been 16\n",
        "times as large (i.e., 28×28×64×1,024, which is roughly 51 million) if we hadn’t used\n",
        "max-pooling.\n",
        "\n",
        "Finally, the output is a fully connected layer with 10 units, corresponding to the number\n",
        "of labels in the dataset (recall that MNIST is a handwritten digit dataset, so the\n",
        "number of possible labels is 10)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1EvgsfvA0K1",
        "colab_type": "code",
        "outputId": "603521a1-ca2a-4ecd-8bc8-2b9617f77fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "DATA_DIR = '/tmp/data'\n",
        "NUM_STEPS = 1000\n",
        "MINIBATCH_SIZE = 100\n",
        "\n",
        "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
        "\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
        "\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(NUM_STEPS):\n",
        "        batch = mnist.train.next_batch(50)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            train_accuracy = sess.run(accuracy, feed_dict={x: batch[0],\n",
        "                                                           y_: batch[1],\n",
        "                                                           keep_prob: 1.0})\n",
        "            print(\"Step {}, training accuracy {}\".format(i, train_accuracy))\n",
        "        \n",
        "        sess.run(train_step, feed_dict={x: batch[0], y_ : batch[1],\n",
        "                                        keep_prob: 0.5})\n",
        "        \n",
        "    X = mnist.test.images.reshape(10, 1000, 784)\n",
        "    Y = mnist.test.labels.reshape(10, 1000, 10)\n",
        "    test_accuracy = np.mean([sess.run(accuracy, feed_dict={x:X[i], y_:Y[i],\n",
        "                                                            keep_prob:1.0}) for i in range(10)])\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-674075c66dd6>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From <ipython-input-5-674075c66dd6>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Step 0, training accuracy 0.03999999910593033\n",
            "Step 100, training accuracy 0.7400000095367432\n",
            "Step 200, training accuracy 0.9599999785423279\n",
            "Step 300, training accuracy 0.8799999952316284\n",
            "Step 400, training accuracy 0.8799999952316284\n",
            "Step 500, training accuracy 0.9399999976158142\n",
            "Step 600, training accuracy 0.9599999785423279\n",
            "Step 700, training accuracy 0.9200000166893005\n",
            "Step 800, training accuracy 0.9599999785423279\n",
            "Step 900, training accuracy 0.9599999785423279\n",
            "Test accuracy: 0.9635000228881836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDwQGzyfLm5n",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98U_sBhHloFA",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrnOHXArlrA7",
        "colab_type": "code",
        "outputId": "a1d6c88d-4e66-4e2b-a5a8-718641fe6b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuLW9iibm3Hy",
        "colab_type": "code",
        "outputId": "d58fe677-eb24-4fd6-95de-a3d3288d50b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"X_train.shape: \", X_train.shape)\n",
        "print(\"X_test.shape: \", X_test.shape)\n",
        "print(\"y_train.shape: \", y_train.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape:  (50000, 32, 32, 3)\n",
            "X_test.shape:  (10000, 32, 32, 3)\n",
            "y_train.shape:  (50000, 1)\n",
            "y_test.shape:  (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSo0Mx6XnKx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}