{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deeplearning-with-keras-chapter1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FZ3O3LOxVQR5","colab_type":"text"},"source":["# Chapter 1: Neural Networks Foundations"]},{"cell_type":"markdown","metadata":{"id":"NyuaHQ89VmXK","colab_type":"text"},"source":["## Perceptron"]},{"cell_type":"markdown","metadata":{"id":"mguF2GZ_WIIs","colab_type":"text"},"source":["f(x) is one if wx + b > 0 and 0 otherwise. Here w is a vector weights and b is a bias. wx + b, defines boundary hyperplane that changes position according to he values assigned to w and b."]},{"cell_type":"markdown","metadata":{"id":"vXfqMDw0XK6p","colab_type":"text"},"source":["### The first example of Keras code"]},{"cell_type":"markdown","metadata":{"id":"5LY-pPcfXN8h","colab_type":"text"},"source":["The initial building block of Keras is a model, and the simplest model is called sequential.\n","A sequential Keras model is a linear pipeline (a stack) of neural networks layers. This code\n","fragment defines a single layer with 12 artificial neurons, and it expects 8 input variables\n","(also known as features):"]},{"cell_type":"code","metadata":{"id":"eVcL_j6zXiXx","colab_type":"code","colab":{}},"source":["# from keras.models import Sequential\n","# model = Sequential()\n","# model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QqT0zjWQX4rh","colab_type":"text"},"source":["The choices for weights initialization:\n","- random_uniform: Weights are initialized to uniformly random small values in\n","(-0.05, 0.05). In other words, any value within the given interval is equally likely\n","to be drawn.\n","- random_normal: Weights are initialized according to a Gaussian, with a zero\n","mean and small standard deviation of 0.05.\n","- zero: All weights are initialized to zero."]},{"cell_type":"markdown","metadata":{"id":"1Za8EsXWYbeC","colab_type":"text"},"source":["## Multilayer Perceptron -- The First Example of a Network"]},{"cell_type":"markdown","metadata":{"id":"1dxoR7n7YjHI","colab_type":"text"},"source":["The net is dense, meaning that each neuron in a layer is connected to all neurons located in the previous layer and to all the neurons in the following layer."]},{"cell_type":"markdown","metadata":{"id":"JM2doTscZclX","colab_type":"text"},"source":["### Problems in training the perceptron and a solution"]},{"cell_type":"markdown","metadata":{"id":"2s7VTcfLZylD","colab_type":"text"},"source":["Ideally, we would like to provide a set of training examples and let the computer adjust the\n","weight and the bias in such a way that the errors produced in the output are minimized. The problem is perceptron is either on or off (one or zero), which makes it have a big output jump. Therefore we need something smoother, we need a function that progressively changes from 0 to 1 with no discontinuity."]},{"cell_type":"markdown","metadata":{"id":"KzZordh7aQbs","colab_type":"text"},"source":["### Activation function -- Sigmoid"]},{"cell_type":"markdown","metadata":{"id":"b3pPopWpbb-F","colab_type":"text"},"source":["$\\sigma(x) = \\frac{1}{1+e^{-x}}$"]},{"cell_type":"markdown","metadata":{"id":"8ljvYIbPboTn","colab_type":"text"},"source":["A neuron can use the sigmoid for computing the nonlinear function $\\sigma(z = wx + b)$. Note that sigmoid function for very large number will close to one and for very large and negative number the function will close to zero. In other words, a neuron with sigmoid activation has a behavior similar to perceptron, but the changes are gradual. It can answer maybe."]},{"cell_type":"markdown","metadata":{"id":"GJjkvW7KcE8d","colab_type":"text"},"source":["### Activation function -- ReLU"]},{"cell_type":"markdown","metadata":{"id":"B-_AZwK-c-4S","colab_type":"text"},"source":["Defined as f(x) = max(0, x)"]},{"cell_type":"markdown","metadata":{"id":"6JaVMISwdVw8","colab_type":"text"},"source":["### Activation functions"]},{"cell_type":"markdown","metadata":{"id":"QZXz8CbxdYw-","colab_type":"text"},"source":["Sigmoid and ReLU are generally called activation functions in neural network jargon. In the\n","Testing different optimizers in Keras section, we will see that those gradual changes, typical of\n","sigmoid and ReLU functions, are the basic building blocks to developing a learning\n","algorithm which adapts little by little, by progressively reducing the mistakes made by our\n","nets."]},{"cell_type":"markdown","metadata":{"id":"Do8qYNMPdjXq","colab_type":"text"},"source":["## A Real Example -- Recognizing Handwritten Digits"]},{"cell_type":"markdown","metadata":{"id":"4xgpqGQId7Hx","colab_type":"text"},"source":["### One-hot-encoding -- OHE"]},{"cell_type":"markdown","metadata":{"id":"h2hA1k2FeaWL","colab_type":"text"},"source":["In many applications, it is convenient to transform categorical (non-numerical) features into\n","numerical variables. For instance, the categorical feature digit with the value d in [0-9] can\n","be encoded into a binary vector with 10 positions, which always has 0 value, except the d-th\n","position where a 1 is present. This type of representation is called one-hot encoding (OHE)\n","and is very common in data mining when the learning algorithm is specialized for dealing\n","with numerical functions."]},{"cell_type":"markdown","metadata":{"id":"tXWvAhWkewkw","colab_type":"text"},"source":["### Defining a simple neural net in Keras"]},{"cell_type":"code","metadata":{"id":"99TeCgGVe0u9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"103caca6-97e6-4126-b0e8-ff526070877e","executionInfo":{"status":"ok","timestamp":1568208071279,"user_tz":0,"elapsed":6456,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["from __future__ import print_function\n","import numpy as np\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Activation\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","np.random.seed(1671) # for reproducibility\n","\n","# network and training\n","NB_EPOCH = 200\n","BATCH_SIZE = 128\n","VERBOSE = 1\n","NB_CLASSES = 10 # the number of outputs == number of digits\n","OPTIMIZER = SGD() # SGD optimizer\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.1 # how much train is reserved for validation\n","\n","# data: shuffled and split between train and test sets\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# X_train is 6000 rows of 28 by 28 values --> reshaped in 60000 x 784\n","RESHAPED = 784\n","\n","X_train = X_train.reshape(60000, RESHAPED)\n","X_test = X_test.reshape(10000, RESHAPED)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","# normalize\n","X_train /= 255\n","X_test /= 255\n","\n","print(X_train.shape[0], \"train samples\")\n","print(X_test.shape[0], \"test samples\")\n","\n","# convert class vectors to binary matrices\n","Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n","Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 2s 0us/step\n","60000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oykoVEPohp6s","colab_type":"text"},"source":["The input layer has a neuron associated with each pixel in the image for a total of 28 x 28 =\n","784 neurons, one for each pixel in the MNIST images.\n","\n","\n","Typically, the values associated with each pixel are normalized in the range [0, 1] (which\n","means that the intensity of each pixel is divided by 255, the maximum intensity value). The\n","output is 10 classes, one for each digit.\n","\n","\n","The final layer is a single neuron with activation function softmax, which is a generalization\n","of the sigmoid function. Softmax squashes a k-dimensional vector of arbitrary real values\n","into a k-dimensional vector of real values in the range (0, 1). In our case, it aggregates 10\n","answers provided by the previous layer with 10 neurons:"]},{"cell_type":"code","metadata":{"id":"Irycq9k8iPCF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":239},"outputId":"d40f8c14-3761-4499-f83c-9d662aff995e","executionInfo":{"status":"ok","timestamp":1568208227244,"user_tz":0,"elapsed":2415,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["# 10 outputs final stage is softmax\n","model = Sequential()\n","model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n","model.add(Activation('softmax'))\n","model.summary()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_2 (Dense)              (None, 10)                7850      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 7,850\n","Trainable params: 7,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T08zpSpOfeP6","colab_type":"text"},"source":["There are a few choices to be made during compilation:\n","- Select the optimizer: algorithm to update the weights\n","- Select the objective function (loss function): used by the optimizer to navigate\n","the space of weights\n","- Evaluate the trained model"]},{"cell_type":"markdown","metadata":{"id":"STKMMyhHf4QG","colab_type":"text"},"source":["Common choices for the objective function:\n","- MSE: Mean squared error. $MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y} - y)^2$\n","- Binary cross-entropy: Binary logarithmic loss. Suitable for binary labels prediction. $Log\\:Loss = -t\\,log(p) - (1 - t)\\,log(1 - p)$. The prediction is p and the target is t.\n","- Categorical cross-entropy: Multiclass logarithmic loss. Suitable for multiclass label predictions. It is also the default choice in association with softmax activation. $L_i = -\\sum_j\\,t_{i,j}\\,log(p_{i,j})$. Where the target is t and prediction is p."]},{"cell_type":"markdown","metadata":{"id":"DBJRDkftg6k8","colab_type":"text"},"source":["Common choices for metrics (used for evaluating the model):\n","- Accuracy: the proportion of correct predictions with respect to the targets\n","- Precision: denotes how many selected items are relevant for a multilabel\n","classification\n","- Recall: denotes how many selected items are relevant for a multilabel\n","classification"]},{"cell_type":"code","metadata":{"id":"HLfR5PdejVkF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":111},"outputId":"baa05ef5-fb41-46b4-ac8f-7ea53a52b4b0","executionInfo":{"status":"ok","timestamp":1568209279575,"user_tz":0,"elapsed":3606,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n","              metrics=['accuracy'])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFuB3Jybjoqt","colab_type":"text"},"source":["Parameters for fit method:\n","- epochs: This is the number of times the model is exposed to the training set. At\n","each iteration, the optimizer tries to adjust the weights so that the objective\n","function is minimized.\n","- batch_size: This is the number of training instances observed before the\n","optimizer performs a weight update."]},{"cell_type":"code","metadata":{"id":"SsC4PVRBj0q4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"27aceeb6-d6a3-4b7d-ac06-2bf65b7e99ef","executionInfo":{"status":"ok","timestamp":1568209811754,"user_tz":0,"elapsed":228797,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["# training the model\n","history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n","                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Train on 54000 samples, validate on 6000 samples\n","Epoch 1/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3091 - acc: 0.9137 - val_loss: 0.2579 - val_acc: 0.9297\n","Epoch 2/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3085 - acc: 0.9135 - val_loss: 0.2574 - val_acc: 0.9293\n","Epoch 3/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3079 - acc: 0.9140 - val_loss: 0.2571 - val_acc: 0.9297\n","Epoch 4/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3074 - acc: 0.9139 - val_loss: 0.2567 - val_acc: 0.9300\n","Epoch 5/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.3069 - acc: 0.9142 - val_loss: 0.2563 - val_acc: 0.9300\n","Epoch 6/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.3064 - acc: 0.9141 - val_loss: 0.2560 - val_acc: 0.9295\n","Epoch 7/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3059 - acc: 0.9146 - val_loss: 0.2556 - val_acc: 0.9298\n","Epoch 8/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.3054 - acc: 0.9147 - val_loss: 0.2552 - val_acc: 0.9298\n","Epoch 9/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3049 - acc: 0.9144 - val_loss: 0.2548 - val_acc: 0.9303\n","Epoch 10/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3044 - acc: 0.9152 - val_loss: 0.2546 - val_acc: 0.9303\n","Epoch 11/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3039 - acc: 0.9150 - val_loss: 0.2544 - val_acc: 0.9302\n","Epoch 12/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.3034 - acc: 0.9150 - val_loss: 0.2540 - val_acc: 0.9308\n","Epoch 13/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.3030 - acc: 0.9153 - val_loss: 0.2536 - val_acc: 0.9302\n","Epoch 14/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.3026 - acc: 0.9151 - val_loss: 0.2532 - val_acc: 0.9305\n","Epoch 15/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3021 - acc: 0.9156 - val_loss: 0.2530 - val_acc: 0.9308\n","Epoch 16/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3017 - acc: 0.9158 - val_loss: 0.2527 - val_acc: 0.9305\n","Epoch 17/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.3013 - acc: 0.9161 - val_loss: 0.2524 - val_acc: 0.9307\n","Epoch 18/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3009 - acc: 0.9160 - val_loss: 0.2522 - val_acc: 0.9310\n","Epoch 19/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3005 - acc: 0.9160 - val_loss: 0.2518 - val_acc: 0.9308\n","Epoch 20/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.3001 - acc: 0.9164 - val_loss: 0.2515 - val_acc: 0.9313\n","Epoch 21/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2997 - acc: 0.9161 - val_loss: 0.2512 - val_acc: 0.9307\n","Epoch 22/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2993 - acc: 0.9165 - val_loss: 0.2510 - val_acc: 0.9310\n","Epoch 23/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2989 - acc: 0.9164 - val_loss: 0.2507 - val_acc: 0.9312\n","Epoch 24/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2986 - acc: 0.9164 - val_loss: 0.2504 - val_acc: 0.9312\n","Epoch 25/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2982 - acc: 0.9166 - val_loss: 0.2504 - val_acc: 0.9312\n","Epoch 26/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2978 - acc: 0.9165 - val_loss: 0.2502 - val_acc: 0.9313\n","Epoch 27/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2975 - acc: 0.9167 - val_loss: 0.2498 - val_acc: 0.9318\n","Epoch 28/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2971 - acc: 0.9171 - val_loss: 0.2495 - val_acc: 0.9315\n","Epoch 29/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2968 - acc: 0.9170 - val_loss: 0.2494 - val_acc: 0.9318\n","Epoch 30/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2965 - acc: 0.9171 - val_loss: 0.2491 - val_acc: 0.9315\n","Epoch 31/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2961 - acc: 0.9173 - val_loss: 0.2490 - val_acc: 0.9322\n","Epoch 32/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2958 - acc: 0.9171 - val_loss: 0.2487 - val_acc: 0.9318\n","Epoch 33/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2955 - acc: 0.9174 - val_loss: 0.2484 - val_acc: 0.9318\n","Epoch 34/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2952 - acc: 0.9174 - val_loss: 0.2483 - val_acc: 0.9327\n","Epoch 35/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2948 - acc: 0.9176 - val_loss: 0.2480 - val_acc: 0.9323\n","Epoch 36/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2945 - acc: 0.9177 - val_loss: 0.2477 - val_acc: 0.9325\n","Epoch 37/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2943 - acc: 0.9178 - val_loss: 0.2475 - val_acc: 0.9322\n","Epoch 38/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2940 - acc: 0.9179 - val_loss: 0.2474 - val_acc: 0.9323\n","Epoch 39/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2937 - acc: 0.9181 - val_loss: 0.2471 - val_acc: 0.9327\n","Epoch 40/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2934 - acc: 0.9178 - val_loss: 0.2470 - val_acc: 0.9335\n","Epoch 41/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2931 - acc: 0.9177 - val_loss: 0.2468 - val_acc: 0.9328\n","Epoch 42/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2928 - acc: 0.9184 - val_loss: 0.2466 - val_acc: 0.9328\n","Epoch 43/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2925 - acc: 0.9183 - val_loss: 0.2465 - val_acc: 0.9328\n","Epoch 44/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2922 - acc: 0.9183 - val_loss: 0.2463 - val_acc: 0.9333\n","Epoch 45/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2920 - acc: 0.9181 - val_loss: 0.2462 - val_acc: 0.9333\n","Epoch 46/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2917 - acc: 0.9183 - val_loss: 0.2459 - val_acc: 0.9333\n","Epoch 47/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2915 - acc: 0.9184 - val_loss: 0.2457 - val_acc: 0.9333\n","Epoch 48/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2912 - acc: 0.9189 - val_loss: 0.2457 - val_acc: 0.9335\n","Epoch 49/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2910 - acc: 0.9187 - val_loss: 0.2454 - val_acc: 0.9333\n","Epoch 50/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2907 - acc: 0.9189 - val_loss: 0.2452 - val_acc: 0.9333\n","Epoch 51/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2904 - acc: 0.9189 - val_loss: 0.2452 - val_acc: 0.9335\n","Epoch 52/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2902 - acc: 0.9189 - val_loss: 0.2451 - val_acc: 0.9337\n","Epoch 53/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2900 - acc: 0.9191 - val_loss: 0.2448 - val_acc: 0.9333\n","Epoch 54/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2897 - acc: 0.9191 - val_loss: 0.2446 - val_acc: 0.9333\n","Epoch 55/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2895 - acc: 0.9192 - val_loss: 0.2445 - val_acc: 0.9335\n","Epoch 56/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2892 - acc: 0.9191 - val_loss: 0.2443 - val_acc: 0.9332\n","Epoch 57/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2890 - acc: 0.9193 - val_loss: 0.2443 - val_acc: 0.9332\n","Epoch 58/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2888 - acc: 0.9196 - val_loss: 0.2439 - val_acc: 0.9340\n","Epoch 59/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2885 - acc: 0.9197 - val_loss: 0.2440 - val_acc: 0.9342\n","Epoch 60/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2883 - acc: 0.9196 - val_loss: 0.2439 - val_acc: 0.9338\n","Epoch 61/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2881 - acc: 0.9196 - val_loss: 0.2437 - val_acc: 0.9340\n","Epoch 62/200\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.2879 - acc: 0.9200 - val_loss: 0.2435 - val_acc: 0.9335\n","Epoch 63/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2877 - acc: 0.9199 - val_loss: 0.2433 - val_acc: 0.9337\n","Epoch 64/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2875 - acc: 0.9197 - val_loss: 0.2434 - val_acc: 0.9342\n","Epoch 65/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2873 - acc: 0.9199 - val_loss: 0.2432 - val_acc: 0.9342\n","Epoch 66/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2870 - acc: 0.9200 - val_loss: 0.2431 - val_acc: 0.9335\n","Epoch 67/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2868 - acc: 0.9199 - val_loss: 0.2428 - val_acc: 0.9342\n","Epoch 68/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2866 - acc: 0.9202 - val_loss: 0.2427 - val_acc: 0.9343\n","Epoch 69/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2864 - acc: 0.9201 - val_loss: 0.2426 - val_acc: 0.9337\n","Epoch 70/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2862 - acc: 0.9200 - val_loss: 0.2424 - val_acc: 0.9340\n","Epoch 71/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2860 - acc: 0.9202 - val_loss: 0.2423 - val_acc: 0.9342\n","Epoch 72/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2858 - acc: 0.9202 - val_loss: 0.2422 - val_acc: 0.9340\n","Epoch 73/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2856 - acc: 0.9206 - val_loss: 0.2420 - val_acc: 0.9337\n","Epoch 74/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2854 - acc: 0.9205 - val_loss: 0.2419 - val_acc: 0.9342\n","Epoch 75/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2852 - acc: 0.9205 - val_loss: 0.2418 - val_acc: 0.9330\n","Epoch 76/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2850 - acc: 0.9207 - val_loss: 0.2417 - val_acc: 0.9340\n","Epoch 77/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2848 - acc: 0.9207 - val_loss: 0.2415 - val_acc: 0.9338\n","Epoch 78/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2847 - acc: 0.9207 - val_loss: 0.2415 - val_acc: 0.9332\n","Epoch 79/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2845 - acc: 0.9209 - val_loss: 0.2414 - val_acc: 0.9343\n","Epoch 80/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2843 - acc: 0.9208 - val_loss: 0.2414 - val_acc: 0.9338\n","Epoch 81/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2841 - acc: 0.9211 - val_loss: 0.2411 - val_acc: 0.9338\n","Epoch 82/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2840 - acc: 0.9209 - val_loss: 0.2409 - val_acc: 0.9333\n","Epoch 83/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2838 - acc: 0.9210 - val_loss: 0.2411 - val_acc: 0.9342\n","Epoch 84/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2836 - acc: 0.9212 - val_loss: 0.2408 - val_acc: 0.9333\n","Epoch 85/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2834 - acc: 0.9209 - val_loss: 0.2408 - val_acc: 0.9337\n","Epoch 86/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2833 - acc: 0.9212 - val_loss: 0.2406 - val_acc: 0.9338\n","Epoch 87/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2831 - acc: 0.9210 - val_loss: 0.2405 - val_acc: 0.9338\n","Epoch 88/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2829 - acc: 0.9213 - val_loss: 0.2404 - val_acc: 0.9340\n","Epoch 89/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2828 - acc: 0.9214 - val_loss: 0.2403 - val_acc: 0.9337\n","Epoch 90/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2826 - acc: 0.9212 - val_loss: 0.2401 - val_acc: 0.9332\n","Epoch 91/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2824 - acc: 0.9214 - val_loss: 0.2399 - val_acc: 0.9342\n","Epoch 92/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2823 - acc: 0.9214 - val_loss: 0.2400 - val_acc: 0.9338\n","Epoch 93/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2821 - acc: 0.9215 - val_loss: 0.2398 - val_acc: 0.9343\n","Epoch 94/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2819 - acc: 0.9213 - val_loss: 0.2401 - val_acc: 0.9345\n","Epoch 95/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2818 - acc: 0.9214 - val_loss: 0.2398 - val_acc: 0.9343\n","Epoch 96/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2816 - acc: 0.9216 - val_loss: 0.2399 - val_acc: 0.9345\n","Epoch 97/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2815 - acc: 0.9217 - val_loss: 0.2395 - val_acc: 0.9338\n","Epoch 98/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2813 - acc: 0.9217 - val_loss: 0.2394 - val_acc: 0.9338\n","Epoch 99/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2812 - acc: 0.9218 - val_loss: 0.2394 - val_acc: 0.9343\n","Epoch 100/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2811 - acc: 0.9219 - val_loss: 0.2393 - val_acc: 0.9342\n","Epoch 101/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2809 - acc: 0.9219 - val_loss: 0.2392 - val_acc: 0.9340\n","Epoch 102/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2807 - acc: 0.9219 - val_loss: 0.2390 - val_acc: 0.9338\n","Epoch 103/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2806 - acc: 0.9221 - val_loss: 0.2390 - val_acc: 0.9340\n","Epoch 104/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2804 - acc: 0.9222 - val_loss: 0.2391 - val_acc: 0.9345\n","Epoch 105/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2803 - acc: 0.9220 - val_loss: 0.2390 - val_acc: 0.9345\n","Epoch 106/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2802 - acc: 0.9222 - val_loss: 0.2388 - val_acc: 0.9342\n","Epoch 107/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2800 - acc: 0.9222 - val_loss: 0.2387 - val_acc: 0.9342\n","Epoch 108/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2798 - acc: 0.9219 - val_loss: 0.2387 - val_acc: 0.9340\n","Epoch 109/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2797 - acc: 0.9222 - val_loss: 0.2386 - val_acc: 0.9340\n","Epoch 110/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2796 - acc: 0.9223 - val_loss: 0.2385 - val_acc: 0.9340\n","Epoch 111/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2795 - acc: 0.9224 - val_loss: 0.2383 - val_acc: 0.9338\n","Epoch 112/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2793 - acc: 0.9223 - val_loss: 0.2383 - val_acc: 0.9340\n","Epoch 113/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2792 - acc: 0.9224 - val_loss: 0.2383 - val_acc: 0.9343\n","Epoch 114/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2791 - acc: 0.9225 - val_loss: 0.2382 - val_acc: 0.9347\n","Epoch 115/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2789 - acc: 0.9225 - val_loss: 0.2381 - val_acc: 0.9342\n","Epoch 116/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2788 - acc: 0.9226 - val_loss: 0.2381 - val_acc: 0.9348\n","Epoch 117/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2786 - acc: 0.9223 - val_loss: 0.2379 - val_acc: 0.9348\n","Epoch 118/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2785 - acc: 0.9227 - val_loss: 0.2378 - val_acc: 0.9340\n","Epoch 119/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2784 - acc: 0.9227 - val_loss: 0.2378 - val_acc: 0.9350\n","Epoch 120/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2782 - acc: 0.9225 - val_loss: 0.2378 - val_acc: 0.9352\n","Epoch 121/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2781 - acc: 0.9228 - val_loss: 0.2376 - val_acc: 0.9342\n","Epoch 122/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2780 - acc: 0.9229 - val_loss: 0.2376 - val_acc: 0.9348\n","Epoch 123/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2779 - acc: 0.9229 - val_loss: 0.2374 - val_acc: 0.9343\n","Epoch 124/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2777 - acc: 0.9229 - val_loss: 0.2374 - val_acc: 0.9350\n","Epoch 125/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2776 - acc: 0.9229 - val_loss: 0.2372 - val_acc: 0.9347\n","Epoch 126/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2775 - acc: 0.9228 - val_loss: 0.2373 - val_acc: 0.9348\n","Epoch 127/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2774 - acc: 0.9227 - val_loss: 0.2371 - val_acc: 0.9345\n","Epoch 128/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2773 - acc: 0.9229 - val_loss: 0.2371 - val_acc: 0.9350\n","Epoch 129/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2771 - acc: 0.9230 - val_loss: 0.2370 - val_acc: 0.9343\n","Epoch 130/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2770 - acc: 0.9229 - val_loss: 0.2370 - val_acc: 0.9350\n","Epoch 131/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2769 - acc: 0.9230 - val_loss: 0.2370 - val_acc: 0.9347\n","Epoch 132/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2768 - acc: 0.9231 - val_loss: 0.2369 - val_acc: 0.9355\n","Epoch 133/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2767 - acc: 0.9230 - val_loss: 0.2368 - val_acc: 0.9347\n","Epoch 134/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2765 - acc: 0.9233 - val_loss: 0.2366 - val_acc: 0.9348\n","Epoch 135/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2764 - acc: 0.9231 - val_loss: 0.2366 - val_acc: 0.9347\n","Epoch 136/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2763 - acc: 0.9232 - val_loss: 0.2366 - val_acc: 0.9348\n","Epoch 137/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2762 - acc: 0.9232 - val_loss: 0.2366 - val_acc: 0.9348\n","Epoch 138/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2761 - acc: 0.9233 - val_loss: 0.2364 - val_acc: 0.9348\n","Epoch 139/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2760 - acc: 0.9235 - val_loss: 0.2366 - val_acc: 0.9348\n","Epoch 140/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2759 - acc: 0.9232 - val_loss: 0.2364 - val_acc: 0.9357\n","Epoch 141/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2758 - acc: 0.9234 - val_loss: 0.2364 - val_acc: 0.9352\n","Epoch 142/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2756 - acc: 0.9235 - val_loss: 0.2363 - val_acc: 0.9352\n","Epoch 143/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2756 - acc: 0.9232 - val_loss: 0.2362 - val_acc: 0.9348\n","Epoch 144/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2754 - acc: 0.9234 - val_loss: 0.2361 - val_acc: 0.9350\n","Epoch 145/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2753 - acc: 0.9234 - val_loss: 0.2361 - val_acc: 0.9353\n","Epoch 146/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2752 - acc: 0.9235 - val_loss: 0.2360 - val_acc: 0.9353\n","Epoch 147/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2751 - acc: 0.9236 - val_loss: 0.2360 - val_acc: 0.9352\n","Epoch 148/200\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.2750 - acc: 0.9236 - val_loss: 0.2358 - val_acc: 0.9353\n","Epoch 149/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2749 - acc: 0.9237 - val_loss: 0.2357 - val_acc: 0.9355\n","Epoch 150/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2748 - acc: 0.9235 - val_loss: 0.2359 - val_acc: 0.9347\n","Epoch 151/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2747 - acc: 0.9238 - val_loss: 0.2358 - val_acc: 0.9353\n","Epoch 152/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2746 - acc: 0.9237 - val_loss: 0.2357 - val_acc: 0.9355\n","Epoch 153/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2745 - acc: 0.9236 - val_loss: 0.2355 - val_acc: 0.9353\n","Epoch 154/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2744 - acc: 0.9236 - val_loss: 0.2356 - val_acc: 0.9355\n","Epoch 155/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2743 - acc: 0.9238 - val_loss: 0.2354 - val_acc: 0.9358\n","Epoch 156/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2742 - acc: 0.9240 - val_loss: 0.2354 - val_acc: 0.9357\n","Epoch 157/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2741 - acc: 0.9239 - val_loss: 0.2353 - val_acc: 0.9348\n","Epoch 158/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2740 - acc: 0.9240 - val_loss: 0.2353 - val_acc: 0.9352\n","Epoch 159/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2739 - acc: 0.9239 - val_loss: 0.2353 - val_acc: 0.9357\n","Epoch 160/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2738 - acc: 0.9238 - val_loss: 0.2352 - val_acc: 0.9350\n","Epoch 161/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2737 - acc: 0.9239 - val_loss: 0.2352 - val_acc: 0.9353\n","Epoch 162/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2736 - acc: 0.9239 - val_loss: 0.2350 - val_acc: 0.9358\n","Epoch 163/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2735 - acc: 0.9240 - val_loss: 0.2350 - val_acc: 0.9358\n","Epoch 164/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2734 - acc: 0.9239 - val_loss: 0.2350 - val_acc: 0.9350\n","Epoch 165/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2733 - acc: 0.9238 - val_loss: 0.2350 - val_acc: 0.9353\n","Epoch 166/200\n","54000/54000 [==============================] - 1s 22us/step - loss: 0.2732 - acc: 0.9240 - val_loss: 0.2350 - val_acc: 0.9352\n","Epoch 167/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2731 - acc: 0.9239 - val_loss: 0.2348 - val_acc: 0.9352\n","Epoch 168/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2730 - acc: 0.9242 - val_loss: 0.2348 - val_acc: 0.9353\n","Epoch 169/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2730 - acc: 0.9240 - val_loss: 0.2348 - val_acc: 0.9358\n","Epoch 170/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2728 - acc: 0.9240 - val_loss: 0.2348 - val_acc: 0.9352\n","Epoch 171/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2727 - acc: 0.9243 - val_loss: 0.2348 - val_acc: 0.9355\n","Epoch 172/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2727 - acc: 0.9241 - val_loss: 0.2346 - val_acc: 0.9353\n","Epoch 173/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2726 - acc: 0.9241 - val_loss: 0.2346 - val_acc: 0.9353\n","Epoch 174/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2725 - acc: 0.9242 - val_loss: 0.2345 - val_acc: 0.9355\n","Epoch 175/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2724 - acc: 0.9243 - val_loss: 0.2344 - val_acc: 0.9358\n","Epoch 176/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2723 - acc: 0.9243 - val_loss: 0.2343 - val_acc: 0.9355\n","Epoch 177/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2722 - acc: 0.9241 - val_loss: 0.2345 - val_acc: 0.9355\n","Epoch 178/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2721 - acc: 0.9243 - val_loss: 0.2345 - val_acc: 0.9357\n","Epoch 179/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2721 - acc: 0.9244 - val_loss: 0.2344 - val_acc: 0.9353\n","Epoch 180/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2720 - acc: 0.9245 - val_loss: 0.2342 - val_acc: 0.9358\n","Epoch 181/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2719 - acc: 0.9243 - val_loss: 0.2342 - val_acc: 0.9350\n","Epoch 182/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2718 - acc: 0.9244 - val_loss: 0.2341 - val_acc: 0.9353\n","Epoch 183/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2717 - acc: 0.9243 - val_loss: 0.2342 - val_acc: 0.9353\n","Epoch 184/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2716 - acc: 0.9244 - val_loss: 0.2341 - val_acc: 0.9353\n","Epoch 185/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2715 - acc: 0.9247 - val_loss: 0.2340 - val_acc: 0.9365\n","Epoch 186/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2714 - acc: 0.9248 - val_loss: 0.2341 - val_acc: 0.9357\n","Epoch 187/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2714 - acc: 0.9245 - val_loss: 0.2340 - val_acc: 0.9355\n","Epoch 188/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2713 - acc: 0.9246 - val_loss: 0.2341 - val_acc: 0.9358\n","Epoch 189/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2712 - acc: 0.9247 - val_loss: 0.2340 - val_acc: 0.9358\n","Epoch 190/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2711 - acc: 0.9248 - val_loss: 0.2339 - val_acc: 0.9353\n","Epoch 191/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2710 - acc: 0.9247 - val_loss: 0.2338 - val_acc: 0.9355\n","Epoch 192/200\n","54000/54000 [==============================] - 1s 20us/step - loss: 0.2710 - acc: 0.9245 - val_loss: 0.2338 - val_acc: 0.9358\n","Epoch 193/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2709 - acc: 0.9245 - val_loss: 0.2338 - val_acc: 0.9355\n","Epoch 194/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2708 - acc: 0.9246 - val_loss: 0.2338 - val_acc: 0.9358\n","Epoch 195/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2707 - acc: 0.9247 - val_loss: 0.2337 - val_acc: 0.9362\n","Epoch 196/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2706 - acc: 0.9246 - val_loss: 0.2336 - val_acc: 0.9358\n","Epoch 197/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2706 - acc: 0.9248 - val_loss: 0.2336 - val_acc: 0.9360\n","Epoch 198/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2705 - acc: 0.9246 - val_loss: 0.2336 - val_acc: 0.9358\n","Epoch 199/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2704 - acc: 0.9247 - val_loss: 0.2335 - val_acc: 0.9348\n","Epoch 200/200\n","54000/54000 [==============================] - 1s 21us/step - loss: 0.2703 - acc: 0.9247 - val_loss: 0.2334 - val_acc: 0.9360\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zWvre_P8kXOU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"9756f805-38ee-4636-af27-d27745984114","executionInfo":{"status":"ok","timestamp":1568210001189,"user_tz":0,"elapsed":3276,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n","print(\"Test score: \", score[0])\n","print(\"Train accuracy: \", score[1])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 0s 35us/step\n","Test score:  0.2716841029793024\n","Train accuracy:  0.9224\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k1IgdmGNmHaT","colab_type":"text"},"source":["### Improving the simple net in Keras with hidden layers"]},{"cell_type":"markdown","metadata":{"id":"SLDvuodOnevp","colab_type":"text"},"source":["A first improvement is to add additional layers to our network. So, after the input layer, we\n","have a first dense layer with the N_HIDDEN neurons and an activation function ReLU. This\n","additional layer is considered hidden because it is not directly connected to either the input\n","or the output. After the first hidden layer, we have a second hidden layer, again with\n","the N_HIDDEN neurons, followed by an output layer with 10 neurons, each of which will fire\n","when the relative digit is recognized. The following code defines this new network:"]},{"cell_type":"code","metadata":{"id":"jsPvQjSCn3QL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"610183ed-160c-4933-dc13-c7d006542362","executionInfo":{"status":"ok","timestamp":1568211037572,"user_tz":0,"elapsed":28612,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["from __future__ import print_function\n","import numpy as np\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Activation\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","np.random.seed(1671) # for reproducibility\n","\n","# network and training\n","NB_EPOCH = 20\n","BATCH_SIZE = 128\n","VERBOSE = 1\n","NB_CLASSES = 10 # the number of outputs == number of digits\n","OPTIMIZER = SGD() # SGD optimizer\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.1 # how much train is reserved for validation\n","\n","# data: shuffled and split between train and test sets\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# X_train is 6000 rows of 28 by 28 values --> reshaped in 60000 x 784\n","RESHAPED = 784\n","\n","X_train = X_train.reshape(60000, RESHAPED)\n","X_test = X_test.reshape(10000, RESHAPED)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","# normalize\n","X_train /= 255\n","X_test /= 255\n","\n","print(X_train.shape[0], \"train samples\")\n","print(X_test.shape[0], \"test samples\")\n","\n","# convert class vectors to binary matrices\n","Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n","Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n","\n","# build the model\n","model = Sequential()\n","model.add(Dense(N_HIDDEN , input_shape=(RESHAPED, )))\n","model.add(Activation('relu'))\n","model.add(Dense(N_HIDDEN))\n","model.add(Activation('relu'))\n","model.add(Dense(NB_CLASSES))\n","model.add(Activation('softmax'))\n","model.summary()\n","\n","# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n","              metrics=['accuracy'])\n","\n","# train the model\n","history = model.fit(X_train, Y_train, epochs=NB_EPOCH, batch_size=BATCH_SIZE,\n","                    validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n","\n","# model evaluation\n","score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n","print(\"Test score: \", score[0])\n","print(\"Test accuracy: \", score[1])\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["60000 train samples\n","10000 test samples\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_3 (Dense)              (None, 128)               100480    \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 10)                1290      \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 54000 samples, validate on 6000 samples\n","Epoch 1/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 1.4073 - acc: 0.6405 - val_loss: 0.6462 - val_acc: 0.8605\n","Epoch 2/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.5610 - acc: 0.8541 - val_loss: 0.3813 - val_acc: 0.9070\n","Epoch 3/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.4171 - acc: 0.8858 - val_loss: 0.3118 - val_acc: 0.9170\n","Epoch 4/20\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.3616 - acc: 0.8980 - val_loss: 0.2758 - val_acc: 0.9245\n","Epoch 5/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3291 - acc: 0.9062 - val_loss: 0.2547 - val_acc: 0.9277\n","Epoch 6/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3063 - acc: 0.9121 - val_loss: 0.2398 - val_acc: 0.9325\n","Epoch 7/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2887 - acc: 0.9165 - val_loss: 0.2272 - val_acc: 0.9347\n","Epoch 8/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2743 - acc: 0.9208 - val_loss: 0.2144 - val_acc: 0.9402\n","Epoch 9/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2617 - acc: 0.9248 - val_loss: 0.2071 - val_acc: 0.9412\n","Epoch 10/20\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.2506 - acc: 0.9285 - val_loss: 0.1985 - val_acc: 0.9440\n","Epoch 11/20\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.2406 - acc: 0.9306 - val_loss: 0.1920 - val_acc: 0.9463\n","Epoch 12/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2319 - acc: 0.9335 - val_loss: 0.1850 - val_acc: 0.9492\n","Epoch 13/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2237 - acc: 0.9354 - val_loss: 0.1774 - val_acc: 0.9523\n","Epoch 14/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2160 - acc: 0.9378 - val_loss: 0.1724 - val_acc: 0.9535\n","Epoch 15/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2088 - acc: 0.9395 - val_loss: 0.1680 - val_acc: 0.9553\n","Epoch 16/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9420 - val_loss: 0.1633 - val_acc: 0.9562\n","Epoch 17/20\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9434 - val_loss: 0.1581 - val_acc: 0.9558\n","Epoch 18/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1902 - acc: 0.9451 - val_loss: 0.1544 - val_acc: 0.9578\n","Epoch 19/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1847 - acc: 0.9469 - val_loss: 0.1510 - val_acc: 0.9582\n","Epoch 20/20\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.1794 - acc: 0.9484 - val_loss: 0.1457 - val_acc: 0.9617\n","10000/10000 [==============================] - 0s 35us/step\n","Test score:  0.17450635524466634\n","Test accuracy:  0.9476\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZBwc9Mp6qPuw","colab_type":"text"},"source":["### Further improving the simple net in Keras with dropout"]},{"cell_type":"markdown","metadata":{"id":"bprKT4RD4NrW","colab_type":"text"},"source":["The second improvement is to randomly drop with the dropout probability some of the values propagated inside our internal dense network of hidden layers."]},{"cell_type":"code","metadata":{"id":"HE_SM0nr4n6u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e3dcaf3d-0873-4ead-ca2e-1d41de77fb13","executionInfo":{"status":"ok","timestamp":1568215103685,"user_tz":0,"elapsed":66241,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["from __future__ import print_function\n","import numpy as np\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","np.random.seed(1671) # for reproducibility\n","\n","# network and training\n","NB_EPOCH = 20\n","BATCH_SIZE = 128\n","VERBOSE = 1\n","NB_CLASSES = 10 # the number of outputs == number of digits\n","OPTIMIZER = SGD() # SGD optimizer\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.1 # how much train is reserved for validation\n","DROPOUT = 0.3\n","\n","# data: shuffled and split between train and test sets\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# X_train is 6000 rows of 28 by 28 values --> reshaped in 60000 x 784\n","RESHAPED = 784\n","\n","X_train = X_train.reshape(60000, RESHAPED)\n","X_test = X_test.reshape(10000, RESHAPED)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","# normalize\n","X_train /= 255\n","X_test /= 255\n","\n","print(X_train.shape[0], \"train samples\")\n","print(X_test.shape[0], \"test samples\")\n","\n","# convert class vectors to binary matrices\n","Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n","Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n","\n","# build the model\n","model = Sequential()\n","model.add(Dense(N_HIDDEN , input_shape=(RESHAPED, )))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(N_HIDDEN))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(NB_CLASSES))\n","model.add(Activation('softmax'))\n","model.summary()\n","\n","# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n","              metrics=['accuracy'])\n","\n","# train the model\n","history = model.fit(X_train, Y_train, epochs=NB_EPOCH, batch_size=BATCH_SIZE,\n","                    validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n","\n","# model evaluation\n","score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n","print(\"Test score: \", score[0])\n","print(\"Test accuracy: \", score[1])\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 24s 2us/step\n","60000 train samples\n","10000 test samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 128)               100480    \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                1290      \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Train on 54000 samples, validate on 6000 samples\n","Epoch 1/20\n","54000/54000 [==============================] - 5s 100us/step - loss: 1.6771 - acc: 0.4760 - val_loss: 0.8042 - val_acc: 0.8490\n","Epoch 2/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.8687 - acc: 0.7355 - val_loss: 0.4603 - val_acc: 0.8890\n","Epoch 3/20\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.6645 - acc: 0.7966 - val_loss: 0.3673 - val_acc: 0.9052\n","Epoch 4/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.5725 - acc: 0.8272 - val_loss: 0.3216 - val_acc: 0.9158\n","Epoch 5/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.5125 - acc: 0.8473 - val_loss: 0.2905 - val_acc: 0.9207\n","Epoch 6/20\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.4733 - acc: 0.8600 - val_loss: 0.2694 - val_acc: 0.9252\n","Epoch 7/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.4445 - acc: 0.8687 - val_loss: 0.2543 - val_acc: 0.9283\n","Epoch 8/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.4205 - acc: 0.8764 - val_loss: 0.2408 - val_acc: 0.9302\n","Epoch 9/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.4020 - acc: 0.8830 - val_loss: 0.2293 - val_acc: 0.9335\n","Epoch 10/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3836 - acc: 0.8868 - val_loss: 0.2187 - val_acc: 0.9363\n","Epoch 11/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.3693 - acc: 0.8922 - val_loss: 0.2098 - val_acc: 0.9398\n","Epoch 12/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3537 - acc: 0.8954 - val_loss: 0.2006 - val_acc: 0.9420\n","Epoch 13/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3419 - acc: 0.8991 - val_loss: 0.1937 - val_acc: 0.9435\n","Epoch 14/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3324 - acc: 0.9012 - val_loss: 0.1871 - val_acc: 0.9460\n","Epoch 15/20\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.3211 - acc: 0.9050 - val_loss: 0.1814 - val_acc: 0.9473\n","Epoch 16/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3100 - acc: 0.9095 - val_loss: 0.1768 - val_acc: 0.9485\n","Epoch 17/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.3060 - acc: 0.9103 - val_loss: 0.1703 - val_acc: 0.9512\n","Epoch 18/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2929 - acc: 0.9153 - val_loss: 0.1653 - val_acc: 0.9525\n","Epoch 19/20\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2877 - acc: 0.9152 - val_loss: 0.1611 - val_acc: 0.9550\n","Epoch 20/20\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2804 - acc: 0.9174 - val_loss: 0.1556 - val_acc: 0.9567\n","10000/10000 [==============================] - 0s 36us/step\n","Test score:  0.18717312158197164\n","Test accuracy:  0.944\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hdzLNxXq5nNy","colab_type":"text"},"source":["Note that training accuracy should still be above the test accuracy, otherwise we are not training long enough."]},{"cell_type":"code","metadata":{"id":"_vqLLZld6EEg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8654bc46-eee7-4aaf-c40a-dfb09ffaa9c1","executionInfo":{"status":"ok","timestamp":1568215552819,"user_tz":0,"elapsed":338732,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["# network and training\n","NB_EPOCH = 20\n","BATCH_SIZE = 128\n","VERBOSE = 1\n","NB_CLASSES = 10 # the number of outputs == number of digits\n","OPTIMIZER = SGD() # SGD optimizer\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.1 # how much train is reserved for validation\n","DROPOUT = 0.3\n","\n","# build the model\n","model = Sequential()\n","model.add(Dense(N_HIDDEN , input_shape=(RESHAPED, )))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(N_HIDDEN))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(NB_CLASSES))\n","model.add(Activation('softmax'))\n","model.summary()\n","\n","# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n","              metrics=['accuracy'])\n","\n","# train the model\n","history = model.fit(X_train, Y_train, epochs=NB_EPOCH, batch_size=BATCH_SIZE,\n","                    validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n","\n","# model evaluation\n","score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n","print(\"Test score: \", score[0])\n","print(\"Test accuracy: \", score[1])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 128)               100480    \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                1290      \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 54000 samples, validate on 6000 samples\n","Epoch 1/250\n","54000/54000 [==============================] - 2s 28us/step - loss: 1.6282 - acc: 0.4808 - val_loss: 0.7711 - val_acc: 0.8412\n","Epoch 2/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.8494 - acc: 0.7351 - val_loss: 0.4538 - val_acc: 0.8892\n","Epoch 3/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.6502 - acc: 0.7999 - val_loss: 0.3589 - val_acc: 0.9062\n","Epoch 4/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.5599 - acc: 0.8283 - val_loss: 0.3119 - val_acc: 0.9145\n","Epoch 5/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.5032 - acc: 0.8487 - val_loss: 0.2836 - val_acc: 0.9212\n","Epoch 6/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.4658 - acc: 0.8602 - val_loss: 0.2625 - val_acc: 0.9255\n","Epoch 7/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.4354 - acc: 0.8702 - val_loss: 0.2466 - val_acc: 0.9295\n","Epoch 8/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.4151 - acc: 0.8765 - val_loss: 0.2329 - val_acc: 0.9328\n","Epoch 9/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3925 - acc: 0.8849 - val_loss: 0.2225 - val_acc: 0.9360\n","Epoch 10/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3777 - acc: 0.8885 - val_loss: 0.2113 - val_acc: 0.9393\n","Epoch 11/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3597 - acc: 0.8939 - val_loss: 0.2028 - val_acc: 0.9408\n","Epoch 12/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3456 - acc: 0.8975 - val_loss: 0.1958 - val_acc: 0.9430\n","Epoch 13/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3334 - acc: 0.9028 - val_loss: 0.1885 - val_acc: 0.9462\n","Epoch 14/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3226 - acc: 0.9056 - val_loss: 0.1800 - val_acc: 0.9490\n","Epoch 15/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3105 - acc: 0.9082 - val_loss: 0.1737 - val_acc: 0.9512\n","Epoch 16/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.3024 - acc: 0.9114 - val_loss: 0.1684 - val_acc: 0.9523\n","Epoch 17/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2957 - acc: 0.9131 - val_loss: 0.1628 - val_acc: 0.9545\n","Epoch 18/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2847 - acc: 0.9169 - val_loss: 0.1577 - val_acc: 0.9572\n","Epoch 19/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2790 - acc: 0.9186 - val_loss: 0.1532 - val_acc: 0.9573\n","Epoch 20/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2744 - acc: 0.9189 - val_loss: 0.1489 - val_acc: 0.9600\n","Epoch 21/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.2656 - acc: 0.9231 - val_loss: 0.1457 - val_acc: 0.9605\n","Epoch 22/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2605 - acc: 0.9229 - val_loss: 0.1417 - val_acc: 0.9625\n","Epoch 23/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2562 - acc: 0.9249 - val_loss: 0.1387 - val_acc: 0.9642\n","Epoch 24/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2480 - acc: 0.9280 - val_loss: 0.1358 - val_acc: 0.9640\n","Epoch 25/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2413 - acc: 0.9289 - val_loss: 0.1331 - val_acc: 0.9648\n","Epoch 26/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2424 - acc: 0.9297 - val_loss: 0.1298 - val_acc: 0.9657\n","Epoch 27/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2348 - acc: 0.9304 - val_loss: 0.1276 - val_acc: 0.9658\n","Epoch 28/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.2327 - acc: 0.9320 - val_loss: 0.1254 - val_acc: 0.9667\n","Epoch 29/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2272 - acc: 0.9337 - val_loss: 0.1227 - val_acc: 0.9680\n","Epoch 30/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.2225 - acc: 0.9340 - val_loss: 0.1200 - val_acc: 0.9685\n","Epoch 31/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.2193 - acc: 0.9359 - val_loss: 0.1182 - val_acc: 0.9688\n","Epoch 32/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2149 - acc: 0.9367 - val_loss: 0.1160 - val_acc: 0.9687\n","Epoch 33/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2149 - acc: 0.9368 - val_loss: 0.1146 - val_acc: 0.9702\n","Epoch 34/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2101 - acc: 0.9388 - val_loss: 0.1135 - val_acc: 0.9702\n","Epoch 35/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2055 - acc: 0.9393 - val_loss: 0.1107 - val_acc: 0.9705\n","Epoch 36/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.2040 - acc: 0.9399 - val_loss: 0.1091 - val_acc: 0.9705\n","Epoch 37/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.2004 - acc: 0.9407 - val_loss: 0.1080 - val_acc: 0.9725\n","Epoch 38/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1960 - acc: 0.9423 - val_loss: 0.1061 - val_acc: 0.9727\n","Epoch 39/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1942 - acc: 0.9422 - val_loss: 0.1060 - val_acc: 0.9723\n","Epoch 40/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1923 - acc: 0.9429 - val_loss: 0.1040 - val_acc: 0.9728\n","Epoch 41/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1918 - acc: 0.9448 - val_loss: 0.1017 - val_acc: 0.9732\n","Epoch 42/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1874 - acc: 0.9443 - val_loss: 0.1020 - val_acc: 0.9733\n","Epoch 43/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1837 - acc: 0.9457 - val_loss: 0.1015 - val_acc: 0.9733\n","Epoch 44/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1846 - acc: 0.9454 - val_loss: 0.0990 - val_acc: 0.9742\n","Epoch 45/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1802 - acc: 0.9461 - val_loss: 0.0976 - val_acc: 0.9748\n","Epoch 46/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1781 - acc: 0.9471 - val_loss: 0.0976 - val_acc: 0.9740\n","Epoch 47/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1769 - acc: 0.9473 - val_loss: 0.0962 - val_acc: 0.9743\n","Epoch 48/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1758 - acc: 0.9481 - val_loss: 0.0953 - val_acc: 0.9747\n","Epoch 49/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1717 - acc: 0.9489 - val_loss: 0.0938 - val_acc: 0.9745\n","Epoch 50/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1732 - acc: 0.9480 - val_loss: 0.0935 - val_acc: 0.9748\n","Epoch 51/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1716 - acc: 0.9483 - val_loss: 0.0924 - val_acc: 0.9752\n","Epoch 52/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1651 - acc: 0.9508 - val_loss: 0.0911 - val_acc: 0.9755\n","Epoch 53/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1637 - acc: 0.9512 - val_loss: 0.0909 - val_acc: 0.9750\n","Epoch 54/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1620 - acc: 0.9528 - val_loss: 0.0899 - val_acc: 0.9750\n","Epoch 55/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1593 - acc: 0.9520 - val_loss: 0.0887 - val_acc: 0.9758\n","Epoch 56/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1617 - acc: 0.9517 - val_loss: 0.0886 - val_acc: 0.9763\n","Epoch 57/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1560 - acc: 0.9533 - val_loss: 0.0877 - val_acc: 0.9757\n","Epoch 58/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1582 - acc: 0.9525 - val_loss: 0.0871 - val_acc: 0.9763\n","Epoch 59/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1544 - acc: 0.9534 - val_loss: 0.0871 - val_acc: 0.9763\n","Epoch 60/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1520 - acc: 0.9550 - val_loss: 0.0862 - val_acc: 0.9760\n","Epoch 61/250\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.1500 - acc: 0.9548 - val_loss: 0.0857 - val_acc: 0.9765\n","Epoch 62/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1500 - acc: 0.9553 - val_loss: 0.0856 - val_acc: 0.9765\n","Epoch 63/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1483 - acc: 0.9555 - val_loss: 0.0847 - val_acc: 0.9758\n","Epoch 64/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1508 - acc: 0.9543 - val_loss: 0.0841 - val_acc: 0.9762\n","Epoch 65/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1474 - acc: 0.9563 - val_loss: 0.0837 - val_acc: 0.9760\n","Epoch 66/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1456 - acc: 0.9568 - val_loss: 0.0829 - val_acc: 0.9763\n","Epoch 67/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1475 - acc: 0.9562 - val_loss: 0.0826 - val_acc: 0.9765\n","Epoch 68/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1425 - acc: 0.9585 - val_loss: 0.0819 - val_acc: 0.9767\n","Epoch 69/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1443 - acc: 0.9567 - val_loss: 0.0810 - val_acc: 0.9772\n","Epoch 70/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1423 - acc: 0.9572 - val_loss: 0.0812 - val_acc: 0.9765\n","Epoch 71/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1369 - acc: 0.9594 - val_loss: 0.0804 - val_acc: 0.9767\n","Epoch 72/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1423 - acc: 0.9570 - val_loss: 0.0795 - val_acc: 0.9765\n","Epoch 73/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1374 - acc: 0.9591 - val_loss: 0.0794 - val_acc: 0.9773\n","Epoch 74/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1390 - acc: 0.9584 - val_loss: 0.0789 - val_acc: 0.9775\n","Epoch 75/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1352 - acc: 0.9599 - val_loss: 0.0789 - val_acc: 0.9780\n","Epoch 76/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1325 - acc: 0.9608 - val_loss: 0.0772 - val_acc: 0.9770\n","Epoch 77/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1334 - acc: 0.9602 - val_loss: 0.0774 - val_acc: 0.9777\n","Epoch 78/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1341 - acc: 0.9600 - val_loss: 0.0771 - val_acc: 0.9775\n","Epoch 79/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1318 - acc: 0.9609 - val_loss: 0.0766 - val_acc: 0.9772\n","Epoch 80/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1309 - acc: 0.9601 - val_loss: 0.0763 - val_acc: 0.9768\n","Epoch 81/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1292 - acc: 0.9615 - val_loss: 0.0756 - val_acc: 0.9782\n","Epoch 82/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1263 - acc: 0.9625 - val_loss: 0.0756 - val_acc: 0.9793\n","Epoch 83/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1271 - acc: 0.9622 - val_loss: 0.0749 - val_acc: 0.9787\n","Epoch 84/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1263 - acc: 0.9616 - val_loss: 0.0761 - val_acc: 0.9785\n","Epoch 85/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1267 - acc: 0.9615 - val_loss: 0.0749 - val_acc: 0.9790\n","Epoch 86/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1269 - acc: 0.9625 - val_loss: 0.0752 - val_acc: 0.9790\n","Epoch 87/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1254 - acc: 0.9622 - val_loss: 0.0744 - val_acc: 0.9783\n","Epoch 88/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1261 - acc: 0.9614 - val_loss: 0.0745 - val_acc: 0.9788\n","Epoch 89/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1222 - acc: 0.9631 - val_loss: 0.0739 - val_acc: 0.9797\n","Epoch 90/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1211 - acc: 0.9641 - val_loss: 0.0736 - val_acc: 0.9783\n","Epoch 91/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1203 - acc: 0.9642 - val_loss: 0.0735 - val_acc: 0.9798\n","Epoch 92/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1211 - acc: 0.9636 - val_loss: 0.0731 - val_acc: 0.9795\n","Epoch 93/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1193 - acc: 0.9637 - val_loss: 0.0731 - val_acc: 0.9793\n","Epoch 94/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1202 - acc: 0.9641 - val_loss: 0.0733 - val_acc: 0.9792\n","Epoch 95/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1145 - acc: 0.9648 - val_loss: 0.0724 - val_acc: 0.9802\n","Epoch 96/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1176 - acc: 0.9648 - val_loss: 0.0723 - val_acc: 0.9800\n","Epoch 97/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1161 - acc: 0.9646 - val_loss: 0.0717 - val_acc: 0.9802\n","Epoch 98/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1170 - acc: 0.9648 - val_loss: 0.0716 - val_acc: 0.9805\n","Epoch 99/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1127 - acc: 0.9670 - val_loss: 0.0711 - val_acc: 0.9802\n","Epoch 100/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1139 - acc: 0.9660 - val_loss: 0.0714 - val_acc: 0.9800\n","Epoch 101/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1101 - acc: 0.9657 - val_loss: 0.0712 - val_acc: 0.9808\n","Epoch 102/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1129 - acc: 0.9658 - val_loss: 0.0707 - val_acc: 0.9805\n","Epoch 103/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1118 - acc: 0.9661 - val_loss: 0.0705 - val_acc: 0.9813\n","Epoch 104/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1108 - acc: 0.9662 - val_loss: 0.0696 - val_acc: 0.9818\n","Epoch 105/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1102 - acc: 0.9679 - val_loss: 0.0706 - val_acc: 0.9813\n","Epoch 106/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1088 - acc: 0.9673 - val_loss: 0.0697 - val_acc: 0.9815\n","Epoch 107/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1078 - acc: 0.9663 - val_loss: 0.0697 - val_acc: 0.9815\n","Epoch 108/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1102 - acc: 0.9659 - val_loss: 0.0698 - val_acc: 0.9813\n","Epoch 109/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1095 - acc: 0.9671 - val_loss: 0.0689 - val_acc: 0.9817\n","Epoch 110/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1067 - acc: 0.9676 - val_loss: 0.0690 - val_acc: 0.9813\n","Epoch 111/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1064 - acc: 0.9670 - val_loss: 0.0682 - val_acc: 0.9817\n","Epoch 112/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1059 - acc: 0.9678 - val_loss: 0.0683 - val_acc: 0.9812\n","Epoch 113/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1072 - acc: 0.9680 - val_loss: 0.0692 - val_acc: 0.9807\n","Epoch 114/250\n","54000/54000 [==============================] - 1s 23us/step - loss: 0.1051 - acc: 0.9684 - val_loss: 0.0683 - val_acc: 0.9810\n","Epoch 115/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1053 - acc: 0.9679 - val_loss: 0.0678 - val_acc: 0.9818\n","Epoch 116/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1048 - acc: 0.9685 - val_loss: 0.0678 - val_acc: 0.9817\n","Epoch 117/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1057 - acc: 0.9685 - val_loss: 0.0680 - val_acc: 0.9818\n","Epoch 118/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1006 - acc: 0.9695 - val_loss: 0.0680 - val_acc: 0.9820\n","Epoch 119/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1009 - acc: 0.9692 - val_loss: 0.0677 - val_acc: 0.9822\n","Epoch 120/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.1028 - acc: 0.9694 - val_loss: 0.0676 - val_acc: 0.9813\n","Epoch 121/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.1028 - acc: 0.9686 - val_loss: 0.0673 - val_acc: 0.9820\n","Epoch 122/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0997 - acc: 0.9695 - val_loss: 0.0667 - val_acc: 0.9822\n","Epoch 123/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0995 - acc: 0.9698 - val_loss: 0.0671 - val_acc: 0.9813\n","Epoch 124/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0989 - acc: 0.9695 - val_loss: 0.0681 - val_acc: 0.9815\n","Epoch 125/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.1002 - acc: 0.9691 - val_loss: 0.0674 - val_acc: 0.9825\n","Epoch 126/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0973 - acc: 0.9694 - val_loss: 0.0675 - val_acc: 0.9813\n","Epoch 127/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0971 - acc: 0.9702 - val_loss: 0.0667 - val_acc: 0.9817\n","Epoch 128/250\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0980 - acc: 0.9704 - val_loss: 0.0665 - val_acc: 0.9827\n","Epoch 129/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0977 - acc: 0.9701 - val_loss: 0.0670 - val_acc: 0.9822\n","Epoch 130/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0970 - acc: 0.9701 - val_loss: 0.0662 - val_acc: 0.9820\n","Epoch 131/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0971 - acc: 0.9700 - val_loss: 0.0661 - val_acc: 0.9828\n","Epoch 132/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0977 - acc: 0.9701 - val_loss: 0.0654 - val_acc: 0.9825\n","Epoch 133/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0961 - acc: 0.9707 - val_loss: 0.0660 - val_acc: 0.9827\n","Epoch 134/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0950 - acc: 0.9708 - val_loss: 0.0655 - val_acc: 0.9822\n","Epoch 135/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0952 - acc: 0.9704 - val_loss: 0.0658 - val_acc: 0.9822\n","Epoch 136/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0938 - acc: 0.9713 - val_loss: 0.0656 - val_acc: 0.9827\n","Epoch 137/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0925 - acc: 0.9708 - val_loss: 0.0656 - val_acc: 0.9830\n","Epoch 138/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0924 - acc: 0.9716 - val_loss: 0.0659 - val_acc: 0.9822\n","Epoch 139/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0903 - acc: 0.9729 - val_loss: 0.0658 - val_acc: 0.9825\n","Epoch 140/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0925 - acc: 0.9717 - val_loss: 0.0653 - val_acc: 0.9825\n","Epoch 141/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0916 - acc: 0.9731 - val_loss: 0.0659 - val_acc: 0.9828\n","Epoch 142/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0919 - acc: 0.9716 - val_loss: 0.0654 - val_acc: 0.9823\n","Epoch 143/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0904 - acc: 0.9728 - val_loss: 0.0656 - val_acc: 0.9823\n","Epoch 144/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0900 - acc: 0.9717 - val_loss: 0.0651 - val_acc: 0.9815\n","Epoch 145/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0905 - acc: 0.9719 - val_loss: 0.0656 - val_acc: 0.9822\n","Epoch 146/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0901 - acc: 0.9713 - val_loss: 0.0644 - val_acc: 0.9823\n","Epoch 147/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0901 - acc: 0.9723 - val_loss: 0.0641 - val_acc: 0.9825\n","Epoch 148/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0880 - acc: 0.9735 - val_loss: 0.0640 - val_acc: 0.9825\n","Epoch 149/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0871 - acc: 0.9732 - val_loss: 0.0639 - val_acc: 0.9825\n","Epoch 150/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0856 - acc: 0.9736 - val_loss: 0.0633 - val_acc: 0.9830\n","Epoch 151/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0871 - acc: 0.9732 - val_loss: 0.0642 - val_acc: 0.9825\n","Epoch 152/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0867 - acc: 0.9735 - val_loss: 0.0640 - val_acc: 0.9823\n","Epoch 153/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0887 - acc: 0.9729 - val_loss: 0.0637 - val_acc: 0.9823\n","Epoch 154/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0850 - acc: 0.9740 - val_loss: 0.0637 - val_acc: 0.9827\n","Epoch 155/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0851 - acc: 0.9744 - val_loss: 0.0639 - val_acc: 0.9825\n","Epoch 156/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0861 - acc: 0.9732 - val_loss: 0.0640 - val_acc: 0.9825\n","Epoch 157/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0849 - acc: 0.9744 - val_loss: 0.0632 - val_acc: 0.9830\n","Epoch 158/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0838 - acc: 0.9736 - val_loss: 0.0635 - val_acc: 0.9822\n","Epoch 159/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0838 - acc: 0.9742 - val_loss: 0.0635 - val_acc: 0.9825\n","Epoch 160/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0838 - acc: 0.9743 - val_loss: 0.0629 - val_acc: 0.9822\n","Epoch 161/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0843 - acc: 0.9735 - val_loss: 0.0632 - val_acc: 0.9827\n","Epoch 162/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0830 - acc: 0.9746 - val_loss: 0.0625 - val_acc: 0.9830\n","Epoch 163/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0810 - acc: 0.9750 - val_loss: 0.0623 - val_acc: 0.9822\n","Epoch 164/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0842 - acc: 0.9743 - val_loss: 0.0629 - val_acc: 0.9832\n","Epoch 165/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0800 - acc: 0.9757 - val_loss: 0.0632 - val_acc: 0.9823\n","Epoch 166/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0811 - acc: 0.9747 - val_loss: 0.0631 - val_acc: 0.9828\n","Epoch 167/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0832 - acc: 0.9744 - val_loss: 0.0626 - val_acc: 0.9828\n","Epoch 168/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0808 - acc: 0.9758 - val_loss: 0.0616 - val_acc: 0.9828\n","Epoch 169/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0810 - acc: 0.9753 - val_loss: 0.0619 - val_acc: 0.9825\n","Epoch 170/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0796 - acc: 0.9747 - val_loss: 0.0628 - val_acc: 0.9827\n","Epoch 171/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0797 - acc: 0.9758 - val_loss: 0.0617 - val_acc: 0.9827\n","Epoch 172/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0801 - acc: 0.9759 - val_loss: 0.0626 - val_acc: 0.9825\n","Epoch 173/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0786 - acc: 0.9759 - val_loss: 0.0626 - val_acc: 0.9828\n","Epoch 174/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0794 - acc: 0.9754 - val_loss: 0.0627 - val_acc: 0.9825\n","Epoch 175/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0795 - acc: 0.9750 - val_loss: 0.0625 - val_acc: 0.9823\n","Epoch 176/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0784 - acc: 0.9759 - val_loss: 0.0621 - val_acc: 0.9828\n","Epoch 177/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0797 - acc: 0.9751 - val_loss: 0.0628 - val_acc: 0.9822\n","Epoch 178/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0768 - acc: 0.9761 - val_loss: 0.0628 - val_acc: 0.9827\n","Epoch 179/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0775 - acc: 0.9764 - val_loss: 0.0621 - val_acc: 0.9828\n","Epoch 180/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0754 - acc: 0.9764 - val_loss: 0.0615 - val_acc: 0.9827\n","Epoch 181/250\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0781 - acc: 0.9759 - val_loss: 0.0620 - val_acc: 0.9832\n","Epoch 182/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0773 - acc: 0.9760 - val_loss: 0.0620 - val_acc: 0.9827\n","Epoch 183/250\n","54000/54000 [==============================] - 1s 28us/step - loss: 0.0786 - acc: 0.9760 - val_loss: 0.0622 - val_acc: 0.9825\n","Epoch 184/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0762 - acc: 0.9762 - val_loss: 0.0618 - val_acc: 0.9822\n","Epoch 185/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0760 - acc: 0.9764 - val_loss: 0.0613 - val_acc: 0.9832\n","Epoch 186/250\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0753 - acc: 0.9758 - val_loss: 0.0613 - val_acc: 0.9820\n","Epoch 187/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0747 - acc: 0.9769 - val_loss: 0.0615 - val_acc: 0.9830\n","Epoch 188/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0744 - acc: 0.9766 - val_loss: 0.0613 - val_acc: 0.9827\n","Epoch 189/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0747 - acc: 0.9765 - val_loss: 0.0615 - val_acc: 0.9830\n","Epoch 190/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0731 - acc: 0.9772 - val_loss: 0.0614 - val_acc: 0.9828\n","Epoch 191/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0752 - acc: 0.9767 - val_loss: 0.0608 - val_acc: 0.9822\n","Epoch 192/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0738 - acc: 0.9770 - val_loss: 0.0617 - val_acc: 0.9825\n","Epoch 193/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0752 - acc: 0.9762 - val_loss: 0.0619 - val_acc: 0.9823\n","Epoch 194/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0712 - acc: 0.9769 - val_loss: 0.0613 - val_acc: 0.9825\n","Epoch 195/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0741 - acc: 0.9768 - val_loss: 0.0619 - val_acc: 0.9822\n","Epoch 196/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0722 - acc: 0.9775 - val_loss: 0.0610 - val_acc: 0.9825\n","Epoch 197/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0749 - acc: 0.9760 - val_loss: 0.0612 - val_acc: 0.9823\n","Epoch 198/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0714 - acc: 0.9785 - val_loss: 0.0603 - val_acc: 0.9828\n","Epoch 199/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0715 - acc: 0.9781 - val_loss: 0.0611 - val_acc: 0.9828\n","Epoch 200/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0718 - acc: 0.9779 - val_loss: 0.0608 - val_acc: 0.9823\n","Epoch 201/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0718 - acc: 0.9774 - val_loss: 0.0612 - val_acc: 0.9828\n","Epoch 202/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0710 - acc: 0.9774 - val_loss: 0.0613 - val_acc: 0.9825\n","Epoch 203/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0719 - acc: 0.9770 - val_loss: 0.0623 - val_acc: 0.9825\n","Epoch 204/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0717 - acc: 0.9770 - val_loss: 0.0615 - val_acc: 0.9825\n","Epoch 205/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0701 - acc: 0.9780 - val_loss: 0.0613 - val_acc: 0.9827\n","Epoch 206/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0696 - acc: 0.9782 - val_loss: 0.0607 - val_acc: 0.9833\n","Epoch 207/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0678 - acc: 0.9789 - val_loss: 0.0605 - val_acc: 0.9825\n","Epoch 208/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0705 - acc: 0.9778 - val_loss: 0.0602 - val_acc: 0.9827\n","Epoch 209/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0691 - acc: 0.9781 - val_loss: 0.0615 - val_acc: 0.9828\n","Epoch 210/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0704 - acc: 0.9777 - val_loss: 0.0608 - val_acc: 0.9832\n","Epoch 211/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0668 - acc: 0.9795 - val_loss: 0.0611 - val_acc: 0.9828\n","Epoch 212/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0706 - acc: 0.9774 - val_loss: 0.0612 - val_acc: 0.9830\n","Epoch 213/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0695 - acc: 0.9780 - val_loss: 0.0608 - val_acc: 0.9828\n","Epoch 214/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0682 - acc: 0.9790 - val_loss: 0.0610 - val_acc: 0.9833\n","Epoch 215/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0691 - acc: 0.9786 - val_loss: 0.0604 - val_acc: 0.9827\n","Epoch 216/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0667 - acc: 0.9797 - val_loss: 0.0610 - val_acc: 0.9832\n","Epoch 217/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0664 - acc: 0.9799 - val_loss: 0.0614 - val_acc: 0.9828\n","Epoch 218/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0678 - acc: 0.9784 - val_loss: 0.0609 - val_acc: 0.9828\n","Epoch 219/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0693 - acc: 0.9786 - val_loss: 0.0607 - val_acc: 0.9830\n","Epoch 220/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0664 - acc: 0.9790 - val_loss: 0.0606 - val_acc: 0.9823\n","Epoch 221/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0655 - acc: 0.9796 - val_loss: 0.0612 - val_acc: 0.9825\n","Epoch 222/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0667 - acc: 0.9791 - val_loss: 0.0607 - val_acc: 0.9827\n","Epoch 223/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0658 - acc: 0.9798 - val_loss: 0.0598 - val_acc: 0.9825\n","Epoch 224/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0686 - acc: 0.9779 - val_loss: 0.0595 - val_acc: 0.9833\n","Epoch 225/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0664 - acc: 0.9789 - val_loss: 0.0603 - val_acc: 0.9828\n","Epoch 226/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0649 - acc: 0.9800 - val_loss: 0.0607 - val_acc: 0.9835\n","Epoch 227/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0652 - acc: 0.9794 - val_loss: 0.0602 - val_acc: 0.9828\n","Epoch 228/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0669 - acc: 0.9791 - val_loss: 0.0598 - val_acc: 0.9833\n","Epoch 229/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0663 - acc: 0.9794 - val_loss: 0.0594 - val_acc: 0.9828\n","Epoch 230/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0664 - acc: 0.9793 - val_loss: 0.0603 - val_acc: 0.9842\n","Epoch 231/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0655 - acc: 0.9790 - val_loss: 0.0608 - val_acc: 0.9828\n","Epoch 232/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0645 - acc: 0.9798 - val_loss: 0.0608 - val_acc: 0.9832\n","Epoch 233/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0644 - acc: 0.9803 - val_loss: 0.0602 - val_acc: 0.9830\n","Epoch 234/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0647 - acc: 0.9796 - val_loss: 0.0596 - val_acc: 0.9827\n","Epoch 235/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0639 - acc: 0.9799 - val_loss: 0.0608 - val_acc: 0.9825\n","Epoch 236/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0622 - acc: 0.9803 - val_loss: 0.0603 - val_acc: 0.9827\n","Epoch 237/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0635 - acc: 0.9796 - val_loss: 0.0601 - val_acc: 0.9823\n","Epoch 238/250\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0638 - acc: 0.9802 - val_loss: 0.0609 - val_acc: 0.9828\n","Epoch 239/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0634 - acc: 0.9800 - val_loss: 0.0609 - val_acc: 0.9823\n","Epoch 240/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0641 - acc: 0.9801 - val_loss: 0.0607 - val_acc: 0.9827\n","Epoch 241/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0609 - acc: 0.9805 - val_loss: 0.0598 - val_acc: 0.9833\n","Epoch 242/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0642 - acc: 0.9793 - val_loss: 0.0607 - val_acc: 0.9827\n","Epoch 243/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0624 - acc: 0.9805 - val_loss: 0.0596 - val_acc: 0.9833\n","Epoch 244/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0624 - acc: 0.9805 - val_loss: 0.0597 - val_acc: 0.9835\n","Epoch 245/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0598 - acc: 0.9809 - val_loss: 0.0605 - val_acc: 0.9828\n","Epoch 246/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0626 - acc: 0.9804 - val_loss: 0.0602 - val_acc: 0.9832\n","Epoch 247/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0615 - acc: 0.9809 - val_loss: 0.0602 - val_acc: 0.9835\n","Epoch 248/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0598 - acc: 0.9809 - val_loss: 0.0606 - val_acc: 0.9828\n","Epoch 249/250\n","54000/54000 [==============================] - 1s 24us/step - loss: 0.0601 - acc: 0.9811 - val_loss: 0.0603 - val_acc: 0.9835\n","Epoch 250/250\n","54000/54000 [==============================] - 1s 25us/step - loss: 0.0594 - acc: 0.9811 - val_loss: 0.0605 - val_acc: 0.9835\n","10000/10000 [==============================] - 0s 36us/step\n","Test score:  0.07021234827675507\n","Test accuracy:  0.9788\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5ra96KHt8OKJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":573},"outputId":"6bd8875e-aca2-400c-9e10-717c78050bd8","executionInfo":{"status":"ok","timestamp":1568215757956,"user_tz":0,"elapsed":2360,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["import matplotlib.pyplot as plt\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiMUFIQEZERSgoiIjLi/wOMaSy5iGeW5AAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3cmZHGhd8QrY","colab_type":"text"},"source":["### Testing different optimizers in Keras"]},{"cell_type":"markdown","metadata":{"id":"dFpBR1sc8eNk","colab_type":"text"},"source":["Gradient Descent Algorithms:\n","$w = w - \\eta\\,(\\frac{\\partial C}{\\partial w})$. The eta $\\eta$ is the learning rate, if to small the process will be slow and if too high, the process will probably miss the minimum loss value. Where C is the cost function or loss function."]},{"cell_type":"markdown","metadata":{"id":"w17pznWw973J","colab_type":"text"},"source":["Once we have the derivative, it is possible to optimize the nets with a gradient descent\n","technique. Keras uses its backend (either TensorFlow or Theano) for computing the\n","derivative on our behalf so we don't need to worry about implementing or computing it.\n","We just choose the activation function, and Keras computes its derivative on our behalf."]},{"cell_type":"markdown","metadata":{"id":"jL5J7ELj_yDL","colab_type":"text"},"source":["Keras implements a fast variant of gradient descent known as stochastic gradient descent\n","(SGD) and two more advanced optimization techniques known as RMSprop and Adam.\n","RMSprop and Adam include the concept of momentum (a velocity component) in addition\n","to the acceleration component that SGD has. This allows faster convergence at the cost of\n","more computation."]},{"cell_type":"code","metadata":{"id":"C0GI58RCAZZW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"993aa5b2-29e1-4891-9fe4-2c62ca63799d","executionInfo":{"status":"ok","timestamp":1568216944455,"user_tz":0,"elapsed":32848,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["from keras.optimizers import RMSprop, Adam\n","\n","# network and training\n","NB_EPOCH = 20\n","BATCH_SIZE = 128\n","VERBOSE = 1\n","NB_CLASSES = 10 # the number of outputs == number of digits\n","OPTIMIZER = RMSprop() # SGD optimizer\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.1 # how much train is reserved for validation\n","DROPOUT = 0.3\n","\n","# build the model\n","model = Sequential()\n","model.add(Dense(N_HIDDEN , input_shape=(RESHAPED, )))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(N_HIDDEN))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(NB_CLASSES))\n","model.add(Activation('softmax'))\n","model.summary()\n","\n","# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n","              metrics=['accuracy'])\n","\n","# train the model\n","history = model.fit(X_train, Y_train, epochs=NB_EPOCH, batch_size=BATCH_SIZE,\n","                    validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n","\n","# model evaluation\n","score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n","print(\"Test score: \", score[0])\n","print(\"Test accuracy: \", score[1])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 128)               100480    \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 10)                1290      \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 54000 samples, validate on 6000 samples\n","Epoch 1/20\n","54000/54000 [==============================] - 2s 34us/step - loss: 0.4583 - acc: 0.8624 - val_loss: 0.1540 - val_acc: 0.9565\n","Epoch 2/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.2172 - acc: 0.9350 - val_loss: 0.1078 - val_acc: 0.9673\n","Epoch 3/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.1670 - acc: 0.9497 - val_loss: 0.0898 - val_acc: 0.9725\n","Epoch 4/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.1454 - acc: 0.9574 - val_loss: 0.0796 - val_acc: 0.9758\n","Epoch 5/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.1299 - acc: 0.9612 - val_loss: 0.0761 - val_acc: 0.9778\n","Epoch 6/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.1181 - acc: 0.9651 - val_loss: 0.0795 - val_acc: 0.9768\n","Epoch 7/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.1090 - acc: 0.9677 - val_loss: 0.0712 - val_acc: 0.9800\n","Epoch 8/20\n","54000/54000 [==============================] - 1s 28us/step - loss: 0.1043 - acc: 0.9695 - val_loss: 0.0740 - val_acc: 0.9807\n","Epoch 9/20\n","54000/54000 [==============================] - 1s 28us/step - loss: 0.0981 - acc: 0.9707 - val_loss: 0.0765 - val_acc: 0.9802\n","Epoch 10/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.0918 - acc: 0.9725 - val_loss: 0.0737 - val_acc: 0.9807\n","Epoch 11/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.0866 - acc: 0.9741 - val_loss: 0.0754 - val_acc: 0.9792\n","Epoch 12/20\n","54000/54000 [==============================] - 1s 26us/step - loss: 0.0878 - acc: 0.9747 - val_loss: 0.0714 - val_acc: 0.9830\n","Epoch 13/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.0831 - acc: 0.9757 - val_loss: 0.0786 - val_acc: 0.9828\n","Epoch 14/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0801 - acc: 0.9765 - val_loss: 0.0804 - val_acc: 0.9818\n","Epoch 15/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0806 - acc: 0.9766 - val_loss: 0.0800 - val_acc: 0.9802\n","Epoch 16/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0801 - acc: 0.9773 - val_loss: 0.0755 - val_acc: 0.9820\n","Epoch 17/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0742 - acc: 0.9793 - val_loss: 0.0718 - val_acc: 0.9830\n","Epoch 18/20\n","54000/54000 [==============================] - 1s 28us/step - loss: 0.0744 - acc: 0.9785 - val_loss: 0.0768 - val_acc: 0.9825\n","Epoch 19/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.0748 - acc: 0.9793 - val_loss: 0.0839 - val_acc: 0.9825\n","Epoch 20/20\n","54000/54000 [==============================] - 1s 27us/step - loss: 0.0720 - acc: 0.9803 - val_loss: 0.0795 - val_acc: 0.9815\n","10000/10000 [==============================] - 0s 39us/step\n","Test score:  0.09692086060072506\n","Test accuracy:  0.9803\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PbdPcLPIA_r3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":573},"outputId":"6c54a49b-7065-4a3d-c022-e5bf1e241e9b","executionInfo":{"status":"ok","timestamp":1568216982637,"user_tz":0,"elapsed":7505,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["import matplotlib.pyplot as plt\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"1HYylEHxBAWc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a4b0f859-6c64-455d-ff04-7e55eae416ce","executionInfo":{"status":"ok","timestamp":1568217079266,"user_tz":0,"elapsed":34514,"user":{"displayName":"Agus Richard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCdxcw2RRXfwWMEoM0jDvA8ljp2FicR8M6QscNCSg=s64","userId":"10794883854614772832"}}},"source":["from keras.optimizers import RMSprop, Adam\n","\n","# network and training\n","NB_EPOCH = 20\n","BATCH_SIZE = 128\n","VERBOSE = 1\n","NB_CLASSES = 10 # the number of outputs == number of digits\n","OPTIMIZER = Adam() # SGD optimizer\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.1 # how much train is reserved for validation\n","DROPOUT = 0.3\n","\n","# build the model\n","model = Sequential()\n","model.add(Dense(N_HIDDEN , input_shape=(RESHAPED, )))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(N_HIDDEN))\n","model.add(Activation('relu'))\n","model.add(Dropout(DROPOUT))\n","model.add(Dense(NB_CLASSES))\n","model.add(Activation('softmax'))\n","model.summary()\n","\n","# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, \n","              metrics=['accuracy'])\n","\n","# train the model\n","history = model.fit(X_train, Y_train, epochs=NB_EPOCH, batch_size=BATCH_SIZE,\n","                    validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n","\n","# model evaluation\n","score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n","print(\"Test score: \", score[0])\n","print(\"Test accuracy: \", score[1])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_10 (Dense)             (None, 128)               100480    \n","_________________________________________________________________\n","activation_10 (Activation)   (None, 128)               0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 128)               16512     \n","_________________________________________________________________\n","activation_11 (Activation)   (None, 128)               0         \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 10)                1290      \n","_________________________________________________________________\n","activation_12 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 54000 samples, validate on 6000 samples\n","Epoch 1/20\n","54000/54000 [==============================] - 2s 36us/step - loss: 0.4842 - acc: 0.8518 - val_loss: 0.1474 - val_acc: 0.9565\n","Epoch 2/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.2268 - acc: 0.9332 - val_loss: 0.1031 - val_acc: 0.9702\n","Epoch 3/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.1739 - acc: 0.9466 - val_loss: 0.0865 - val_acc: 0.9732\n","Epoch 4/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.1442 - acc: 0.9560 - val_loss: 0.0869 - val_acc: 0.9752\n","Epoch 5/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.1265 - acc: 0.9617 - val_loss: 0.0768 - val_acc: 0.9773\n","Epoch 6/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.1145 - acc: 0.9649 - val_loss: 0.0684 - val_acc: 0.9797\n","Epoch 7/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.1072 - acc: 0.9675 - val_loss: 0.0715 - val_acc: 0.9793\n","Epoch 8/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.0945 - acc: 0.9703 - val_loss: 0.0695 - val_acc: 0.9793\n","Epoch 9/20\n","54000/54000 [==============================] - 2s 28us/step - loss: 0.0864 - acc: 0.9733 - val_loss: 0.0734 - val_acc: 0.9780\n","Epoch 10/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.0824 - acc: 0.9741 - val_loss: 0.0661 - val_acc: 0.9807\n","Epoch 11/20\n","54000/54000 [==============================] - 2s 32us/step - loss: 0.0807 - acc: 0.9744 - val_loss: 0.0684 - val_acc: 0.9792\n","Epoch 12/20\n","54000/54000 [==============================] - 2s 31us/step - loss: 0.0749 - acc: 0.9758 - val_loss: 0.0691 - val_acc: 0.9807\n","Epoch 13/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.0710 - acc: 0.9771 - val_loss: 0.0628 - val_acc: 0.9817\n","Epoch 14/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.0698 - acc: 0.9776 - val_loss: 0.0706 - val_acc: 0.9812\n","Epoch 15/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.0663 - acc: 0.9786 - val_loss: 0.0677 - val_acc: 0.9783\n","Epoch 16/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.0606 - acc: 0.9805 - val_loss: 0.0695 - val_acc: 0.9798\n","Epoch 17/20\n","54000/54000 [==============================] - 2s 32us/step - loss: 0.0608 - acc: 0.9807 - val_loss: 0.0616 - val_acc: 0.9823\n","Epoch 18/20\n","54000/54000 [==============================] - 2s 30us/step - loss: 0.0594 - acc: 0.9806 - val_loss: 0.0694 - val_acc: 0.9800\n","Epoch 19/20\n","54000/54000 [==============================] - 2s 31us/step - loss: 0.0602 - acc: 0.9800 - val_loss: 0.0603 - val_acc: 0.9832\n","Epoch 20/20\n","54000/54000 [==============================] - 2s 29us/step - loss: 0.0541 - acc: 0.9829 - val_loss: 0.0654 - val_acc: 0.9805\n","10000/10000 [==============================] - 0s 37us/step\n","Test score:  0.07833279383834743\n","Test accuracy:  0.9798\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HgBWyo8yBxXp","colab_type":"text"},"source":["### Increasing the number of epochs"]},{"cell_type":"markdown","metadata":{"id":"j4cpIhwmCm6v","colab_type":"text"},"source":["Learning is more about adopting smart techniques and not necessarily about the time spent in computations."]},{"cell_type":"markdown","metadata":{"id":"1VhOoLvkCynI","colab_type":"text"},"source":["Another technique to improve the performance:\n","- Controlling the optimizer learning rate: Find the sweet spot between low and high value for learning rate\n","- Increasing the number of internal hidden neurons: Will increase the training time and parameters to update.\n","- Increasing the size of the batch computation"]},{"cell_type":"markdown","metadata":{"id":"3Xt7YY5ADT_L","colab_type":"text"},"source":["### Adopting regularization for avoiding overfitting"]},{"cell_type":"markdown","metadata":{"id":"UwSRHWxLF4j0","colab_type":"text"},"source":["min : {loss(Training Data | Model)}"]},{"cell_type":"markdown","metadata":{"id":"sTXCnHy8ZDmY","colab_type":"text"},"source":["As a rule of thumb, if during the training we see that the loss increases on validation, after\n","an initial decrease, then we have a problem of model complexity that overfits training.\n","Indeed, overfitting is the word used in machine learning for concisely describing this\n","phenomenon.\n","\n","We can use a hyperparameter lambda >= 0 for controlling what the importance of having\n","a simple model is, as in this formula:\n","\n","min: {loss(Training data|model)} + lambda * complexity(model)\n"]},{"cell_type":"markdown","metadata":{"id":"EpIoaa7Li3mI","colab_type":"text"},"source":["There are three different types of regularizations used in machine learning:\n","- L1 regularization (also known as lasso): The complexity of the model is\n","expressed as the sum of the absolute values of the weights\n","- L2 regularization (also known as ridge): The complexity of the model is\n","expressed as the sum of the squares of the weights\n","- Elastic net regularization: The complexity of the model is captured by a\n","combination of the two preceding techniques"]},{"cell_type":"markdown","metadata":{"id":"giRxZt3yi7kx","colab_type":"text"},"source":["Therefore, playing with regularization can be a good way to increase the performance of a\n","network, in particular when there is an evident situation of overfitting."]},{"cell_type":"code","metadata":{"id":"rrEvgEySjs_L","colab_type":"code","colab":{}},"source":["# # adding l2 regularization for kernel\n","# from keras import regularizers\n","# model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RG2E4pIOkKEj","colab_type":"text"},"source":["### Hyperparameters tuning"]},{"cell_type":"markdown","metadata":{"id":"wgaE7XDHkcx8","colab_type":"text"},"source":["Hyperparameters tuning is the process of finding the optimal combinations of those parameters that minimize cost functions."]},{"cell_type":"markdown","metadata":{"id":"wYsXnA7IlCwD","colab_type":"text"},"source":["### Predicting output"]},{"cell_type":"code","metadata":{"id":"BBJWUFC0lF0G","colab_type":"code","colab":{}},"source":["# # calculate predictions\n","# predictions = model.predict(X)\n","\n","# # for a given input, several types of output can be computed, including a method:\n","# model.evaluate() # used to compute the loss values\n","# model.predict_classes() # used to compute category outputs\n","# model.predict_proba() # used to compute class probabilities"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxDspoq_llV3","colab_type":"text"},"source":["## A practical overview of backpropagation\n","\n","In the beginning, all the weights have some random assignment. Then the net is activated\n","for each input in the training set: values are propagated forward from the input stage\n","through the hidden stages to the output stage where a prediction is made (note that we\n","have kept the following diagram simple by only representing a few values with green\n","dotted lines, but in reality, all the values are propagated forward through the network).\n","\n","Since we know the true observed value in the training set, it is possible to calculate the error\n","made in prediction. The key intuition for backtracking is to propagate the error back and\n","use an appropriate optimizer algorithm, such as a gradient descent, to adjust the neural\n","network weights with the goal of reducing the error.\n","\n","The features represent the input and the labels are here used to drive the learning process.\n","The model is updated in such a way that the loss function is progressively minimized. In a\n","neural network, what really matters is not the output of a single neuron but the collective\n","weights adjusted in each layer. Therefore, the network progressively adjusts its internal\n","weights in such a way that the prediction increases the number of labels correctly\n","forecasted. Of course, using the right set features and having a quality labeled data is\n","fundamental to minimizing the bias during the learning process.\n"]}]}